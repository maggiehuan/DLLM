Encoder CNN shapes: {'image': (64, 64, 3)}
Encoder MLP shapes: {'transition_tokens': (384,)}
Decoder CNN shapes: {'image': (64, 64, 3)}
Decoder MLP shapes: {'transition_tokens': (384,)}
JAX devices (1): [cuda(id=0)]
Policy devices: cuda:0
Train devices:  cuda:0
Tracing train function.
no rnd data in data
Optimizer model_opt has 197,057,283 variables.
Optimizer actor_opt has 9,464,849 variables.
Optimizer critic_opt has 9,708,799 variables.
Logdir /home/ziyu/logdir/ziyu_crafter_cuda_2_seed_2
Observation space:
  image            Space(dtype=uint8, shape=(64, 64, 3), low=0, high=255)
  transition_tokens Space(dtype=uint32, shape=(384,), low=0, high=4294967295)
  goal_tokens      Space(dtype=uint32, shape=(5, 384), low=0, high=4294967295)
  goal_id          Space(dtype=uint32, shape=(5,), low=0, high=4294967295)
  reward           Space(dtype=float32, shape=(), low=-inf, high=inf)
  is_first         Space(dtype=bool, shape=(), low=False, high=True)
  is_last          Space(dtype=bool, shape=(), low=False, high=True)
  is_terminal      Space(dtype=bool, shape=(), low=False, high=True)
  log_reward       Space(dtype=float32, shape=(1,), low=-inf, high=inf)
  log_achievement_collect_coal Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_diamond Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_drink Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_iron Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_sapling Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_wood Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_skeleton Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_zombie Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_cow Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_furnace Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_table Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_wake_up Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
Action space:
  action           Space(dtype=float32, shape=(17,), low=0, high=1)
  reset            Space(dtype=bool, shape=(), low=False, high=True)
Prefill train dataset.
Prefill eval dataset.
Found existing checkpoint.
Loading checkpoint: /home/ziyu/logdir/ziyu_crafter_cuda_2_seed_2/checkpoint.ckpt
Loaded checkpoint from 36343 seconds ago.
Start training loop.
Starting evaluation at step 24300
Tracing policy function.
Tracing policy function.
Episode has 118 steps and return 3.1.
Tracing policy function.
Tracing train function.
Tracing report function.
Tracing report function.
Tracing report function.
────────────────────────────────── Step 24301 ──────────────────────────────────
eval_episode/length 118 / eval_episode/score 3.1 / eval_episode/reward_rate 0.97
/ eval_stats/sum_log_reward 3.1 / eval_stats/max_log_achievement_collect_sapling
2 / eval_stats/max_log_achievement_collect_wood 1 /
eval_stats/max_log_achievement_place_plant 2 /
eval_stats/max_log_achievement_wake_up 1 / train/action_mag 16 /
train/action_max 16 / train/action_mean 5.9 / train/action_min 0 /
train/action_std 4.32 / train/actor_opt_actor_opt_grad_overflow 0 /
train/actor_opt_actor_opt_grad_scale 1e4 / train/actor_opt_grad_norm 0.05 /
train/actor_opt_grad_steps 1.2e4 / train/actor_opt_loss -27.99 / train/adv_mag
0.76 / train/adv_max 0.76 / train/adv_mean -3.5e-3 / train/adv_min -0.59 /
train/adv_std 0.07 / train/cont_avg 0.99 / train/cont_loss_mean 3.6e-6 /
train/cont_loss_std 5.8e-5 / train/cont_neg_acc 1 / train/cont_neg_loss 1e-4 /
train/cont_pos_acc 1 / train/cont_pos_loss 2.7e-6 / train/cont_pred 0.99 /
train/cont_rate 0.99 / train/dyn_loss_mean 5.31 / train/dyn_loss_std 11.43 /
train/extr_critic_critic_opt_critic_opt_grad_overflow 0 /
train/extr_critic_critic_opt_critic_opt_grad_scale 1e4 /
train/extr_critic_critic_opt_grad_norm 1.35 /
train/extr_critic_critic_opt_grad_steps 1.2e4 /
train/extr_critic_critic_opt_loss 1.6e4 / train/extr_critic_mag 7.97 /
train/extr_critic_max 7.97 / train/extr_critic_mean 1.83 / train/extr_critic_min
-0.62 / train/extr_critic_std 1.92 / train/extr_return_normed_mag 1.51 /
train/extr_return_normed_max 1.51 / train/extr_return_normed_mean 0.37 /
train/extr_return_normed_min -0.14 / train/extr_return_normed_std 0.35 /
train/extr_return_rate 0.63 / train/extr_return_raw_mag 8.24 /
train/extr_return_raw_max 8.24 / train/extr_return_raw_mean 1.81 /
train/extr_return_raw_min -1.03 / train/extr_return_raw_std 1.95 /
train/extr_reward_mag 1.01 / train/extr_reward_max 1.01 / train/extr_reward_mean
0.02 / train/extr_reward_min -0.71 / train/extr_reward_std 0.15 /
train/image_loss_mean 9.86 / train/image_loss_std 29.87 / train/model_loss_mean
13.09 / train/model_loss_std 35.39 / train/model_opt_grad_norm 76.7 /
train/model_opt_grad_steps 1.2e4 / train/model_opt_loss 1.6e4 /
train/model_opt_model_opt_grad_overflow 0 / train/model_opt_model_opt_grad_scale
1250 / train/policy_entropy_mag 2.44 / train/policy_entropy_max 2.44 /
train/policy_entropy_mean 0.5 / train/policy_entropy_min 0.08 /
train/policy_entropy_std 0.53 / train/policy_logprob_mag 7.44 /
train/policy_logprob_max -9.5e-3 / train/policy_logprob_mean -0.5 /
train/policy_logprob_min -7.44 / train/policy_logprob_std 1.1 /
train/policy_randomness_mag 0.86 / train/policy_randomness_max 0.86 /
train/policy_randomness_mean 0.18 / train/policy_randomness_min 0.03 /
train/policy_randomness_std 0.19 / train/post_ent_mag 38.57 / train/post_ent_max
38.57 / train/post_ent_mean 22.73 / train/post_ent_min 8.92 / train/post_ent_std
5.52 / train/prior_ent_mag 68.1 / train/prior_ent_max 68.1 /
train/prior_ent_mean 27.02 / train/prior_ent_min 11.59 / train/prior_ent_std
10.36 / train/rep_loss_mean 5.31 / train/rep_loss_std 11.43 / train/reward_avg
9.3e-3 / train/reward_loss_mean 0.05 / train/reward_loss_std 0.28 /
train/reward_max_data 1 / train/reward_max_pred 1 / train/reward_neg_acc 1 /
train/reward_neg_loss 0.04 / train/reward_pos_acc 1 / train/reward_pos_loss 0.69
/ train/reward_pred 0.01 / train/reward_rate 0.02 /
train/transition_tokens_loss_mean 5.4e-4 / train/transition_tokens_loss_std
1.7e-5 / train/params_agent/wm/model_opt 2e8 /
train/params_agent/task_behavior/critic/critic_opt 9.7e6 /
train/params_agent/task_behavior/ac/actor_opt 9.5e6 / report/cont_avg 0.99 /
report/cont_loss_mean 3.3e-6 / report/cont_loss_std 3.8e-5 / report/cont_neg_acc
1 / report/cont_neg_loss 1.6e-4 / report/cont_pos_acc 1 / report/cont_pos_loss
2e-6 / report/cont_pred 0.99 / report/cont_rate 0.99 / report/dyn_loss_mean 4.84
/ report/dyn_loss_std 10.41 / report/image_loss_mean 6.92 /
report/image_loss_std 18.2 / report/model_loss_mean 9.87 / report/model_loss_std
23.17 / report/post_ent_mag 40.79 / report/post_ent_max 40.79 /
report/post_ent_mean 22.78 / report/post_ent_min 8.81 / report/post_ent_std 5.54
/ report/prior_ent_mag 68.17 / report/prior_ent_max 68.17 /
report/prior_ent_mean 26.99 / report/prior_ent_min 11.96 / report/prior_ent_std
10.35 / report/rep_loss_mean 4.84 / report/rep_loss_std 10.41 /
report/reward_avg 9.3e-3 / report/reward_loss_mean 0.04 / report/reward_loss_std
0.22 / report/reward_max_data 1 / report/reward_max_pred 1 /
report/reward_neg_acc 1 / report/reward_neg_loss 0.03 / report/reward_pos_acc 1
/ report/reward_pos_loss 0.68 / report/reward_pred 0.01 / report/reward_rate
0.02 / report/transition_tokens_loss_mean 6.2e-4 /
report/transition_tokens_loss_std 1.9e-5 / eval/cont_avg 1 / eval/cont_loss_mean
9.6e-4 / eval/cont_loss_std 0.03 / eval/cont_neg_acc 0.67 / eval/cont_neg_loss
0.33 / eval/cont_pos_acc 1 / eval/cont_pos_loss 5.1e-6 / eval/cont_pred 1 /
eval/cont_rate 1 / eval/dyn_loss_mean 23.46 / eval/dyn_loss_std 16.38 /
eval/image_loss_mean 39.93 / eval/image_loss_std 57.69 / eval/model_loss_mean
54.13 / eval/model_loss_std 64.68 / eval/post_ent_mag 37.65 / eval/post_ent_max
37.65 / eval/post_ent_mean 24.29 / eval/post_ent_min 7.87 / eval/post_ent_std
6.19 / eval/prior_ent_mag 68.17 / eval/prior_ent_max 68.17 / eval/prior_ent_mean
31 / eval/prior_ent_min 9.28 / eval/prior_ent_std 10.72 / eval/rep_loss_mean
23.46 / eval/rep_loss_std 16.38 / eval/reward_avg 0.01 / eval/reward_loss_mean
0.12 / eval/reward_loss_std 0.82 / eval/reward_max_data 1 / eval/reward_max_pred
1 / eval/reward_neg_acc 1 / eval/reward_neg_loss 0.06 / eval/reward_pos_acc 0.59
/ eval/reward_pos_loss 3.42 / eval/reward_pred 5.9e-3 / eval/reward_rate 0.02 /
eval/transition_tokens_loss_mean 6.1e-4 / eval/transition_tokens_loss_std 1.6e-5
/ replay/size 6.1e4 / replay/inserts 0 / replay/samples 112 /
replay/insert_wait_avg nan / replay/insert_wait_frac nan /
replay/sample_wait_avg 1.6e-6 / replay/sample_wait_frac 1 / eval_replay/size
2652 / eval_replay/inserts 56 / eval_replay/samples 112 /
eval_replay/insert_wait_avg 3.9e-6 / eval_replay/insert_wait_frac 1 /
eval_replay/sample_wait_avg 1.5e-6 / eval_replay/sample_wait_frac 1 /
timer/duration 163.32 / timer/replay._sample_count 112 /
timer/replay._sample_total 7.93 / timer/replay._sample_frac 0.05 /
timer/replay._sample_avg 0.07 / timer/replay._sample_min 7.5e-4 /
timer/replay._sample_max 0.17 / timer/agent.policy_count 120 /
timer/agent.policy_total 8.25 / timer/agent.policy_frac 0.05 /
timer/agent.policy_avg 0.07 / timer/agent.policy_min 8.7e-3 /
timer/agent.policy_max 5.12 / timer/env.step_count 1 / timer/env.step_total 1.48
/ timer/env.step_frac 9e-3 / timer/env.step_avg 1.48 / timer/env.step_min 1.48 /
timer/env.step_max 1.48 / timer/dataset_train_count 1 /
timer/dataset_train_total 6.9e-5 / timer/dataset_train_frac 4.2e-7 /
timer/dataset_train_avg 6.9e-5 / timer/dataset_train_min 6.9e-5 /
timer/dataset_train_max 6.9e-5 / timer/agent.train_count 1 /
timer/agent.train_total 101.51 / timer/agent.train_frac 0.62 /
timer/agent.train_avg 101.51 / timer/agent.train_min 101.51 /
timer/agent.train_max 101.51 / timer/agent.report_count 2 /
timer/agent.report_total 22.21 / timer/agent.report_frac 0.14 /
timer/agent.report_avg 11.11 / timer/agent.report_min 6.18 /
timer/agent.report_max 16.03 / timer/dataset_eval_count 1 /
timer/dataset_eval_total 9.5e-5 / timer/dataset_eval_frac 5.8e-7 /
timer/dataset_eval_avg 9.5e-5 / timer/dataset_eval_min 9.5e-5 /
timer/dataset_eval_max 9.5e-5
Creating new TensorBoard event file writer.
