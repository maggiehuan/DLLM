parsed:  
Config:
configs:  [crafter]  (strs) 
remaining: ['--run.script', 'train_eval', '--use_wandb', 'True', '--logdir', '/home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3']
parsed:  
Config:
use_wandb:                  True                                          (bool)
seed:                       1                                             (int)
method:                     name                                          (str)
task:                       crafter_reward                                (str)
logdir:                     /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3  (str)
replay:                     uniform                                       (str)
replay_size:                1000000.0                                     (float)
replay_online:              False                                         (bool)
eval_dir:                                                                 (str)
filter:                     .*                                            (str)
jax.platform:               gpu                                           (str)
jax.jit:                    True                                          (bool)
jax.precision:              float16                                       (str)
jax.prealloc:               True                                          (bool)
jax.debug_nans:             False                                         (bool)
jax.logical_cpus:           0                                             (int)
jax.debug:                  False                                         (bool)
jax.policy_devices:         [0]                                           (ints)
jax.train_devices:          [0]                                           (ints)
jax.metrics_every:          10                                            (int)
run.script:                 train_eval                                    (str)
run.steps:                  10000000000.0                                 (float)
run.expl_until:             0                                             (int)
run.log_every:              300                                           (int)
run.save_every:             900                                           (int)
run.eval_every:             1000000.0                                     (float)
run.eval_initial:           True                                          (bool)
run.eval_eps:               1                                             (int)
run.eval_samples:           1                                             (int)
run.train_ratio:            512.0                                         (float)
run.train_fill:             0                                             (int)
run.eval_fill:              0                                             (int)
run.log_zeros:              False                                         (bool)
run.log_keys_video:         [image]                                       (strs)
run.log_keys_sum:           ^log_reward$                                  (str)
run.log_keys_mean:          (log_entropy)                                 (str)
run.log_keys_max:           ^log_achievement_.*                           (str)
run.from_checkpoint:                                                      (str)
run.sync_every:             10                                            (int)
run.actor_addr:             ipc:///tmp/5551                               (str)
run.actor_batch:            32                                            (int)
envs.amount:                1                                             (int)
envs.parallel:              process                                       (str)
envs.length:                0                                             (int)
envs.reset:                 True                                          (bool)
envs.restart:               True                                          (bool)
envs.discretize:            0                                             (int)
envs.checks:                False                                         (bool)
wrapper.length:             0                                             (int)
wrapper.reset:              True                                          (bool)
wrapper.discretize:         0                                             (int)
wrapper.checks:             False                                         (bool)
env.atari.size:             [64, 64]                                      (ints)
env.atari.repeat:           4                                             (int)
env.atari.sticky:           True                                          (bool)
env.atari.gray:             False                                         (bool)
env.atari.actions:          all                                           (str)
env.atari.lives:            unused                                        (str)
env.atari.noops:            0                                             (int)
env.atari.resize:           opencv                                        (str)
env.dmlab.size:             [64, 64]                                      (ints)
env.dmlab.repeat:           4                                             (int)
env.dmlab.episodic:         True                                          (bool)
env.minecraft.size:         [64, 64]                                      (ints)
env.minecraft.break_speed:  100.0                                         (float)
env.dmc.size:               [64, 64]                                      (ints)
env.dmc.repeat:             2                                             (int)
env.dmc.camera:             -1                                            (int)
env.loconav.size:           [64, 64]                                      (ints)
env.loconav.repeat:         2                                             (int)
env.loconav.camera:         -1                                            (int)
task_behavior:              Greedy                                        (str)
expl_behavior:              None                                          (str)
batch_size:                 16                                            (int)
batch_length:               64                                            (int)
data_loaders:               8                                             (int)
grad_heads:                 [decoder, reward, cont]                       (strs)
rssm.deter:                 4096                                          (int)
rssm.units:                 1024                                          (int)
rssm.stoch:                 32                                            (int)
rssm.classes:               32                                            (int)
rssm.act:                   silu                                          (str)
rssm.norm:                  layer                                         (str)
rssm.initial:               learned                                       (str)
rssm.unimix:                0.01                                          (float)
rssm.unroll:                False                                         (bool)
rssm.action_clip:           1.0                                           (float)
rssm.winit:                 normal                                        (str)
rssm.fan:                   avg                                           (str)
encoder.mlp_keys:           transition_tokens                             (str)
encoder.cnn_keys:           image                                         (str)
encoder.act:                silu                                          (str)
encoder.norm:               layer                                         (str)
encoder.mlp_layers:         5                                             (int)
encoder.mlp_units:          1024                                          (int)
encoder.cnn:                resnet                                        (str)
encoder.cnn_depth:          96                                            (int)
encoder.cnn_blocks:         0                                             (int)
encoder.resize:             stride                                        (str)
encoder.winit:              normal                                        (str)
encoder.fan:                avg                                           (str)
encoder.symlog_inputs:      True                                          (bool)
encoder.minres:             4                                             (int)
decoder.mlp_keys:           transition_tokens                             (str)
decoder.cnn_keys:           image                                         (str)
decoder.act:                silu                                          (str)
decoder.norm:               layer                                         (str)
decoder.mlp_layers:         5                                             (int)
decoder.mlp_units:          1024                                          (int)
decoder.cnn:                resnet                                        (str)
decoder.cnn_depth:          96                                            (int)
decoder.cnn_blocks:         0                                             (int)
decoder.image_dist:         mse                                           (str)
decoder.vector_dist:        symlog_mse                                    (str)
decoder.inputs:             [deter, stoch]                                (strs)
decoder.resize:             stride                                        (str)
decoder.winit:              normal                                        (str)
decoder.fan:                avg                                           (str)
decoder.outscale:           1.0                                           (float)
decoder.minres:             4                                             (int)
decoder.cnn_sigmoid:        False                                         (bool)
reward_head.layers:         5                                             (int)
reward_head.units:          1024                                          (int)
reward_head.act:            silu                                          (str)
reward_head.norm:           layer                                         (str)
reward_head.dist:           symlog_disc                                   (str)
reward_head.outscale:       0.0                                           (float)
reward_head.outnorm:        False                                         (bool)
reward_head.inputs:         [deter, stoch]                                (strs)
reward_head.winit:          normal                                        (str)
reward_head.fan:            avg                                           (str)
reward_head.bins:           255                                           (int)
cont_head.layers:           5                                             (int)
cont_head.units:            1024                                          (int)
cont_head.act:              silu                                          (str)
cont_head.norm:             layer                                         (str)
cont_head.dist:             binary                                        (str)
cont_head.outscale:         1.0                                           (float)
cont_head.outnorm:          False                                         (bool)
cont_head.inputs:           [deter, stoch]                                (strs)
cont_head.winit:            normal                                        (str)
cont_head.fan:              avg                                           (str)
loss_scales.image:          1.0                                           (float)
loss_scales.vector:         1.0                                           (float)
loss_scales.reward:         1.0                                           (float)
loss_scales.cont:           1.0                                           (float)
loss_scales.dyn:            0.5                                           (float)
loss_scales.rep:            0.1                                           (float)
loss_scales.actor:          1.0                                           (float)
loss_scales.critic:         1.0                                           (float)
loss_scales.slowreg:        1.0                                           (float)
dyn_loss.impl:              kl                                            (str)
dyn_loss.free:              1.0                                           (float)
rep_loss.impl:              kl                                            (str)
rep_loss.free:              1.0                                           (float)
model_opt.opt:              adam                                          (str)
model_opt.lr:               0.0001                                        (float)
model_opt.eps:              1e-08                                         (float)
model_opt.clip:             1000.0                                        (float)
model_opt.wd:               0.0                                           (float)
model_opt.warmup:           0                                             (int)
model_opt.lateclip:         0.0                                           (float)
actor.layers:               5                                             (int)
actor.units:                1024                                          (int)
actor.act:                  silu                                          (str)
actor.norm:                 layer                                         (str)
actor.minstd:               0.1                                           (float)
actor.maxstd:               1.0                                           (float)
actor.outscale:             1.0                                           (float)
actor.outnorm:              False                                         (bool)
actor.unimix:               0.01                                          (float)
actor.inputs:               [deter, stoch]                                (strs)
actor.winit:                normal                                        (str)
actor.fan:                  avg                                           (str)
actor.symlog_inputs:        False                                         (bool)
critic.layers:              5                                             (int)
critic.units:               1024                                          (int)
critic.act:                 silu                                          (str)
critic.norm:                layer                                         (str)
critic.dist:                symlog_disc                                   (str)
critic.outscale:            0.0                                           (float)
critic.outnorm:             False                                         (bool)
critic.inputs:              [deter, stoch]                                (strs)
critic.winit:               normal                                        (str)
critic.fan:                 avg                                           (str)
critic.bins:                255                                           (int)
critic.symlog_inputs:       False                                         (bool)
actor_opt.opt:              adam                                          (str)
actor_opt.lr:               3e-05                                         (float)
actor_opt.eps:              1e-05                                         (float)
actor_opt.clip:             100.0                                         (float)
actor_opt.wd:               0.0                                           (float)
actor_opt.warmup:           0                                             (int)
actor_opt.lateclip:         0.0                                           (float)
critic_opt.opt:             adam                                          (str)
critic_opt.lr:              3e-05                                         (float)
critic_opt.eps:             1e-05                                         (float)
critic_opt.clip:            100.0                                         (float)
critic_opt.wd:              0.0                                           (float)
critic_opt.warmup:          0                                             (int)
critic_opt.lateclip:        0.0                                           (float)
actor_dist_disc:            onehot                                        (str)
actor_dist_cont:            normal                                        (str)
actor_grad_disc:            reinforce                                     (str)
actor_grad_cont:            backprop                                      (str)
critic_type:                vfunction                                     (str)
imag_horizon:               15                                            (int)
imag_unroll:                False                                         (bool)
horizon:                    333                                           (int)
return_lambda:              0.95                                          (float)
critic_slowreg:             logprob                                       (str)
slow_critic_update:         1                                             (int)
slow_critic_fraction:       0.02                                          (float)
retnorm.impl:               perc_ema                                      (str)
retnorm.decay:              0.99                                          (float)
retnorm.max:                1.0                                           (float)
retnorm.perclo:             5.0                                           (float)
retnorm.perchi:             95.0                                          (float)
actent:                     0.0003                                        (float)
expl_rewards.extr:          1.0                                           (float)
expl_rewards.disag:         0.1                                           (float)
expl_opt.opt:               adam                                          (str)
expl_opt.lr:                0.0001                                        (float)
expl_opt.eps:               1e-05                                         (float)
expl_opt.clip:              100.0                                         (float)
expl_opt.wd:                0.0                                           (float)
expl_opt.warmup:            0                                             (int)
disag_head.layers:          5                                             (int)
disag_head.units:           1024                                          (int)
disag_head.act:             silu                                          (str)
disag_head.norm:            layer                                         (str)
disag_head.dist:            mse                                           (str)
disag_head.outscale:        1.0                                           (float)
disag_head.inputs:          [deter, stoch, action]                        (strs)
disag_head.winit:           normal                                        (str)
disag_head.fan:             avg                                           (str)
disag_target:               [stoch]                                       (strs)
disag_models:               8                                             (int) 
remaining: []

Config:
use_wandb:                  True                                          (bool)
seed:                       1                                             (int)
method:                     name                                          (str)
task:                       crafter_reward                                (str)
logdir:                     /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3  (str)
replay:                     uniform                                       (str)
replay_size:                1000000.0                                     (float)
replay_online:              False                                         (bool)
eval_dir:                                                                 (str)
filter:                     .*                                            (str)
jax.platform:               gpu                                           (str)
jax.jit:                    True                                          (bool)
jax.precision:              float16                                       (str)
jax.prealloc:               True                                          (bool)
jax.debug_nans:             False                                         (bool)
jax.logical_cpus:           0                                             (int)
jax.debug:                  False                                         (bool)
jax.policy_devices:         [0]                                           (ints)
jax.train_devices:          [0]                                           (ints)
jax.metrics_every:          10                                            (int)
run.script:                 train_eval                                    (str)
run.steps:                  10000000000.0                                 (float)
run.expl_until:             0                                             (int)
run.log_every:              300                                           (int)
run.save_every:             900                                           (int)
run.eval_every:             1000000.0                                     (float)
run.eval_initial:           True                                          (bool)
run.eval_eps:               1                                             (int)
run.eval_samples:           1                                             (int)
run.train_ratio:            512.0                                         (float)
run.train_fill:             0                                             (int)
run.eval_fill:              0                                             (int)
run.log_zeros:              False                                         (bool)
run.log_keys_video:         [image]                                       (strs)
run.log_keys_sum:           ^log_reward$                                  (str)
run.log_keys_mean:          (log_entropy)                                 (str)
run.log_keys_max:           ^log_achievement_.*                           (str)
run.from_checkpoint:                                                      (str)
run.sync_every:             10                                            (int)
run.actor_addr:             ipc:///tmp/5551                               (str)
run.actor_batch:            32                                            (int)
envs.amount:                1                                             (int)
envs.parallel:              process                                       (str)
envs.length:                0                                             (int)
envs.reset:                 True                                          (bool)
envs.restart:               True                                          (bool)
envs.discretize:            0                                             (int)
envs.checks:                False                                         (bool)
wrapper.length:             0                                             (int)
wrapper.reset:              True                                          (bool)
wrapper.discretize:         0                                             (int)
wrapper.checks:             False                                         (bool)
env.atari.size:             [64, 64]                                      (ints)
env.atari.repeat:           4                                             (int)
env.atari.sticky:           True                                          (bool)
env.atari.gray:             False                                         (bool)
env.atari.actions:          all                                           (str)
env.atari.lives:            unused                                        (str)
env.atari.noops:            0                                             (int)
env.atari.resize:           opencv                                        (str)
env.dmlab.size:             [64, 64]                                      (ints)
env.dmlab.repeat:           4                                             (int)
env.dmlab.episodic:         True                                          (bool)
env.minecraft.size:         [64, 64]                                      (ints)
env.minecraft.break_speed:  100.0                                         (float)
env.dmc.size:               [64, 64]                                      (ints)
env.dmc.repeat:             2                                             (int)
env.dmc.camera:             -1                                            (int)
env.loconav.size:           [64, 64]                                      (ints)
env.loconav.repeat:         2                                             (int)
env.loconav.camera:         -1                                            (int)
task_behavior:              Greedy                                        (str)
expl_behavior:              None                                          (str)
batch_size:                 16                                            (int)
batch_length:               64                                            (int)
data_loaders:               8                                             (int)
grad_heads:                 [decoder, reward, cont]                       (strs)
rssm.deter:                 4096                                          (int)
rssm.units:                 1024                                          (int)
rssm.stoch:                 32                                            (int)
rssm.classes:               32                                            (int)
rssm.act:                   silu                                          (str)
rssm.norm:                  layer                                         (str)
rssm.initial:               learned                                       (str)
rssm.unimix:                0.01                                          (float)
rssm.unroll:                False                                         (bool)
rssm.action_clip:           1.0                                           (float)
rssm.winit:                 normal                                        (str)
rssm.fan:                   avg                                           (str)
encoder.mlp_keys:           transition_tokens                             (str)
encoder.cnn_keys:           image                                         (str)
encoder.act:                silu                                          (str)
encoder.norm:               layer                                         (str)
encoder.mlp_layers:         5                                             (int)
encoder.mlp_units:          1024                                          (int)
encoder.cnn:                resnet                                        (str)
encoder.cnn_depth:          96                                            (int)
encoder.cnn_blocks:         0                                             (int)
encoder.resize:             stride                                        (str)
encoder.winit:              normal                                        (str)
encoder.fan:                avg                                           (str)
encoder.symlog_inputs:      True                                          (bool)
encoder.minres:             4                                             (int)
decoder.mlp_keys:           transition_tokens                             (str)
decoder.cnn_keys:           image                                         (str)
decoder.act:                silu                                          (str)
decoder.norm:               layer                                         (str)
decoder.mlp_layers:         5                                             (int)
decoder.mlp_units:          1024                                          (int)
decoder.cnn:                resnet                                        (str)
decoder.cnn_depth:          96                                            (int)
decoder.cnn_blocks:         0                                             (int)
decoder.image_dist:         mse                                           (str)
decoder.vector_dist:        symlog_mse                                    (str)
decoder.inputs:             [deter, stoch]                                (strs)
decoder.resize:             stride                                        (str)
decoder.winit:              normal                                        (str)
decoder.fan:                avg                                           (str)
decoder.outscale:           1.0                                           (float)
decoder.minres:             4                                             (int)
decoder.cnn_sigmoid:        False                                         (bool)
reward_head.layers:         5                                             (int)
reward_head.units:          1024                                          (int)
reward_head.act:            silu                                          (str)
reward_head.norm:           layer                                         (str)
reward_head.dist:           symlog_disc                                   (str)
reward_head.outscale:       0.0                                           (float)
reward_head.outnorm:        False                                         (bool)
reward_head.inputs:         [deter, stoch]                                (strs)
reward_head.winit:          normal                                        (str)
reward_head.fan:            avg                                           (str)
reward_head.bins:           255                                           (int)
cont_head.layers:           5                                             (int)
cont_head.units:            1024                                          (int)
cont_head.act:              silu                                          (str)
cont_head.norm:             layer                                         (str)
cont_head.dist:             binary                                        (str)
cont_head.outscale:         1.0                                           (float)
cont_head.outnorm:          False                                         (bool)
cont_head.inputs:           [deter, stoch]                                (strs)
cont_head.winit:            normal                                        (str)
cont_head.fan:              avg                                           (str)
loss_scales.image:          1.0                                           (float)
loss_scales.vector:         1.0                                           (float)
loss_scales.reward:         1.0                                           (float)
loss_scales.cont:           1.0                                           (float)
loss_scales.dyn:            0.5                                           (float)
loss_scales.rep:            0.1                                           (float)
loss_scales.actor:          1.0                                           (float)
loss_scales.critic:         1.0                                           (float)
loss_scales.slowreg:        1.0                                           (float)
dyn_loss.impl:              kl                                            (str)
dyn_loss.free:              1.0                                           (float)
rep_loss.impl:              kl                                            (str)
rep_loss.free:              1.0                                           (float)
model_opt.opt:              adam                                          (str)
model_opt.lr:               0.0001                                        (float)
model_opt.eps:              1e-08                                         (float)
model_opt.clip:             1000.0                                        (float)
model_opt.wd:               0.0                                           (float)
model_opt.warmup:           0                                             (int)
model_opt.lateclip:         0.0                                           (float)
actor.layers:               5                                             (int)
actor.units:                1024                                          (int)
actor.act:                  silu                                          (str)
actor.norm:                 layer                                         (str)
actor.minstd:               0.1                                           (float)
actor.maxstd:               1.0                                           (float)
actor.outscale:             1.0                                           (float)
actor.outnorm:              False                                         (bool)
actor.unimix:               0.01                                          (float)
actor.inputs:               [deter, stoch]                                (strs)
actor.winit:                normal                                        (str)
actor.fan:                  avg                                           (str)
actor.symlog_inputs:        False                                         (bool)
critic.layers:              5                                             (int)
critic.units:               1024                                          (int)
critic.act:                 silu                                          (str)
critic.norm:                layer                                         (str)
critic.dist:                symlog_disc                                   (str)
critic.outscale:            0.0                                           (float)
critic.outnorm:             False                                         (bool)
critic.inputs:              [deter, stoch]                                (strs)
critic.winit:               normal                                        (str)
critic.fan:                 avg                                           (str)
critic.bins:                255                                           (int)
critic.symlog_inputs:       False                                         (bool)
actor_opt.opt:              adam                                          (str)
actor_opt.lr:               3e-05                                         (float)
actor_opt.eps:              1e-05                                         (float)
actor_opt.clip:             100.0                                         (float)
actor_opt.wd:               0.0                                           (float)
actor_opt.warmup:           0                                             (int)
actor_opt.lateclip:         0.0                                           (float)
critic_opt.opt:             adam                                          (str)
critic_opt.lr:              3e-05                                         (float)
critic_opt.eps:             1e-05                                         (float)
critic_opt.clip:            100.0                                         (float)
critic_opt.wd:              0.0                                           (float)
critic_opt.warmup:          0                                             (int)
critic_opt.lateclip:        0.0                                           (float)
actor_dist_disc:            onehot                                        (str)
actor_dist_cont:            normal                                        (str)
actor_grad_disc:            reinforce                                     (str)
actor_grad_cont:            backprop                                      (str)
critic_type:                vfunction                                     (str)
imag_horizon:               15                                            (int)
imag_unroll:                False                                         (bool)
horizon:                    333                                           (int)
return_lambda:              0.95                                          (float)
critic_slowreg:             logprob                                       (str)
slow_critic_update:         1                                             (int)
slow_critic_fraction:       0.02                                          (float)
retnorm.impl:               perc_ema                                      (str)
retnorm.decay:              0.99                                          (float)
retnorm.max:                1.0                                           (float)
retnorm.perclo:             5.0                                           (float)
retnorm.perchi:             95.0                                          (float)
actent:                     0.0003                                        (float)
expl_rewards.extr:          1.0                                           (float)
expl_rewards.disag:         0.1                                           (float)
expl_opt.opt:               adam                                          (str)
expl_opt.lr:                0.0001                                        (float)
expl_opt.eps:               1e-05                                         (float)
expl_opt.clip:              100.0                                         (float)
expl_opt.wd:                0.0                                           (float)
expl_opt.warmup:            0                                             (int)
disag_head.layers:          5                                             (int)
disag_head.units:           1024                                          (int)
disag_head.act:             silu                                          (str)
disag_head.norm:            layer                                         (str)
disag_head.dist:            mse                                           (str)
disag_head.outscale:        1.0                                           (float)
disag_head.inputs:          [deter, stoch, action]                        (strs)
disag_head.winit:           normal                                        (str)
disag_head.fan:             avg                                           (str)
disag_target:               [stoch]                                       (strs)
disag_models:               8                                             (int)wandb: Currently logged in as: ibisbill326 (ibisbill326-gmail-com). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/wandb/run-20240102_173536-a4qs02g3
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run ziyu_crafter_cuda_3_seed_3
wandb: ⭐️ View project at https://wandb.ai/ibisbill326-gmail-com/crafter_reward
wandb: 🚀 View run at https://wandb.ai/ibisbill326-gmail-com/crafter_reward/runs/a4qs02g3

!! Resuming wandb run !!
Encoder CNN shapes: {'image': (64, 64, 3)}
Encoder MLP shapes: {'transition_tokens': (384,)}
Decoder CNN shapes: {'image': (64, 64, 3)}
Decoder MLP shapes: {'transition_tokens': (384,)}
JAX devices (1): [cuda(id=0)]
Policy devices: cuda:0
Train devices:  cuda:0
Tracing train function.
no rnd data in data
Optimizer model_opt has 197,057,283 variables.
Optimizer actor_opt has 9,464,849 variables.
Optimizer critic_opt has 9,708,799 variables.
Logdir /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3
Observation space:
  image            Space(dtype=uint8, shape=(64, 64, 3), low=0, high=255)
  transition_tokens Space(dtype=uint32, shape=(384,), low=0, high=4294967295)
  goal_tokens      Space(dtype=uint32, shape=(5, 384), low=0, high=4294967295)
  goal_id          Space(dtype=uint32, shape=(5,), low=0, high=4294967295)
  reward           Space(dtype=float32, shape=(), low=-inf, high=inf)
  is_first         Space(dtype=bool, shape=(), low=False, high=True)
  is_last          Space(dtype=bool, shape=(), low=False, high=True)
  is_terminal      Space(dtype=bool, shape=(), low=False, high=True)
  log_reward       Space(dtype=float32, shape=(1,), low=-inf, high=inf)
  log_achievement_collect_coal Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_diamond Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_drink Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_iron Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_sapling Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_wood Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_skeleton Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_zombie Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_cow Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_furnace Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_table Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_wake_up Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
Action space:
  action           Space(dtype=float32, shape=(17,), low=0, high=1)
  reset            Space(dtype=bool, shape=(), low=False, high=True)
Prefill train dataset.
Prefill eval dataset.
Found existing checkpoint.
Loading checkpoint: /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3/checkpoint.ckpt
Loaded checkpoint from 37924 seconds ago.
Start training loop.
Starting evaluation at step 19600
Tracing policy function.
Tracing policy function.
Traceback (most recent call last):
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py", line 229, in <module>
    main()
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py", line 69, in main
    embodied.run.train_eval(
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/train_eval.py", line 145, in train_eval
    driver_eval(policy_eval, episodes=max(len(eval_env), args.eval_eps))
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 42, in __call__
    step, episode = self._step(policy, step, episode)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 47, in _step
    obs = self._env.step(acts)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/batch.py", line 34, in step
    obs = [ob() for ob in obs]
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/batch.py", line 34, in <listcomp>
    obs = [ob() for ob in obs]
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/worker.py", line 230, in __call__
    self._result = self._receive(self._callid)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/worker.py", line 174, in _receive
    message, callid, payload = self._pipe.recv()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py:229 in   │
│ <module>                                                                     │
│                                                                              │
│   226                                                                        │
│   227                                                                        │
│   228 if __name__ == '__main__':                                             │
│ ❱ 229   main()                                                               │
│   230                                                                        │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py:69 in    │
│ main                                                                         │
│                                                                              │
│    66 │     eval_env = make_envs(config)  # mode='eval'                      │
│    67 │     cleanup += [env, eval_env]                                       │
│    68 │     agent = agt.Agent(env.obs_space, env.act_space, step, config)    │
│ ❱  69 │     embodied.run.train_eval(                                         │
│    70 │   │     agent, env, eval_env, replay, eval_replay, logger, args)     │
│    71 │                                                                      │
│    72 │   elif args.script == 'train_holdout':                               │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/trai │
│ n_eval.py:145 in train_eval                                                  │
│                                                                              │
│   142 │   if should_eval(step):                                              │
│   143 │     print('Starting evaluation at step', int(step))                  │
│   144 │     driver_eval.reset()                                              │
│ ❱ 145 │     driver_eval(policy_eval, episodes=max(len(eval_env), args.eval_e │
│   146 │   driver_train(policy_train, steps=100)                              │
│   147 │   if should_save(step):                                              │
│   148 │     checkpoint.save()                                                │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:42 in __call__                                                        │
│                                                                              │
│   39   def __call__(self, policy, steps=0, episodes=0):                      │
│   40 │   step, episode = 0, 0                                                │
│   41 │   while step < steps or episode < episodes:                           │
│ ❱ 42 │     step, episode = self._step(policy, step, episode)                 │
│   43                                                                         │
│   44   def _step(self, policy, step, episode):                               │
│   45 │   assert all(len(x) == len(self._env) for x in self._acts.values())   │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:47 in _step                                                           │
│                                                                              │
│   44   def _step(self, policy, step, episode):                               │
│   45 │   assert all(len(x) == len(self._env) for x in self._acts.values())   │
│   46 │   acts = {k: v for k, v in self._acts.items() if not k.startswith('lo │
│ ❱ 47 │   obs = self._env.step(acts)                                          │
│   48 │   obs = {k: convert(v) for k, v in obs.items()}                       │
│   49 │   assert all(len(x) == len(self._env) for x in obs.values()), obs     │
│   50 │   acts, self._state = policy(obs, self._state, **self._kwargs)        │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/bat │
│ ch.py:34 in step                                                             │
│                                                                              │
│   31 │     act = {k: v[i] for k, v in action.items()}                        │
│   32 │     obs.append(env.step(act))                                         │
│   33 │   if self._parallel:                                                  │
│ ❱ 34 │     obs = [ob() for ob in obs]                                        │
│   35 │   return {k: np.array([ob[k] for ob in obs]) for k in obs[0]}         │
│   36                                                                         │
│   37   def render(self):                                                     │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/bat │
│ ch.py:34 in <listcomp>                                                       │
│                                                                              │
│   31 │     act = {k: v[i] for k, v in action.items()}                        │
│   32 │     obs.append(env.step(act))                                         │
│   33 │   if self._parallel:                                                  │
│ ❱ 34 │     obs = [ob() for ob in obs]                                        │
│   35 │   return {k: np.array([ob[k] for ob in obs]) for k in obs[0]}         │
│   36                                                                         │
│   37   def render(self):                                                     │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/wor │
│ ker.py:230 in __call__                                                       │
│                                                                              │
│   227                                                                        │
│   228   def __call__(self):                                                  │
│   229 │   if not self._complete:                                             │
│ ❱ 230 │     self._result = self._receive(self._callid)                       │
│   231 │     self._complete = True                                            │
│   232 │   return self._result                                                │
│   233                                                                        │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/wor │
│ ker.py:174 in _receive                                                       │
│                                                                              │
│   171   def _receive(self, callid):                                          │
│   172 │   while callid not in self._results:                                 │
│   173 │     try:                                                             │
│ ❱ 174 │   │   message, callid, payload = self._pipe.recv()                   │
│   175 │     except (OSError, EOFError):                                      │
│   176 │   │   raise RuntimeError('Lost connection to worker.')               │
│   177 │     if message == Message.ERROR:                                     │
│                                                                              │
│ /home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/multiprocessing/connection. │
│ py:250 in recv                                                               │
│                                                                              │
│   247 │   │   """Receive a (picklable) object"""                             │
│   248 │   │   self._check_closed()                                           │
│   249 │   │   self._check_readable()                                         │
│ ❱ 250 │   │   buf = self._recv_bytes()                                       │
│   251 │   │   return _ForkingPickler.loads(buf.getbuffer())                  │
│   252 │                                                                      │
│   253 │   def poll(self, timeout=0.0):                                       │
│                                                                              │
│ /home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/multiprocessing/connection. │
│ py:414 in _recv_bytes                                                        │
│                                                                              │
│   411 │   │   │   │   self._send(header + buf)                               │
│   412 │                                                                      │
│   413 │   def _recv_bytes(self, maxsize=None):                               │
│ ❱ 414 │   │   buf = self._recv(4)                                            │
│   415 │   │   size, = struct.unpack("!i", buf.getvalue())                    │
│   416 │   │   if size == -1:                                                 │
│   417 │   │   │   buf = self._recv(8)                                        │
│                                                                              │
│ /home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/multiprocessing/connection. │
│ py:379 in _recv                                                              │
│                                                                              │
│   376 │   │   handle = self._handle                                          │
│   377 │   │   remaining = size                                               │
│   378 │   │   while remaining > 0:                                           │
│ ❱ 379 │   │   │   chunk = read(handle, remaining)                            │
│   380 │   │   │   n = len(chunk)                                             │
│   381 │   │   │   if n == 0:                                                 │
│   382 │   │   │   │   if remaining == size:                                  │
╰──────────────────────────────────────────────────────────────────────────────╯
KeyboardInterrupt
Exception ignored in atexit callback: <function _Manager._atexit_setup.<locals>.<lambda> at 0x7f9ba850ba30>
Traceback (most recent call last):
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 155, in <lambda>
    self._atexit_lambda = lambda: self._atexit_teardown()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 164, in _atexit_teardown
    self._teardown(exit_code)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 175, in _teardown
    result = self._service.join()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/service/service.py", line 271, in join
    ret = self._internal_proc.wait()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt: 
wandb: WARNING No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.000 MB of 0.000 MB uploadedwandb: \ 0.000 MB of 0.023 MB uploadedwandb: 🚀 View run ziyu_crafter_cuda_3_seed_3 at: https://wandb.ai/ibisbill326-gmail-com/crafter_reward/runs/a4qs02g3
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240102_173536-a4qs02g3/logs
parsed:  
Config:
configs:  [crafter]  (strs) 
remaining: ['--run.script', 'train_eval', '--use_wandb', 'True', '--logdir', '/home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3']
parsed:  
Config:
use_wandb:                  True                                          (bool)
seed:                       1                                             (int)
method:                     name                                          (str)
task:                       crafter_reward                                (str)
logdir:                     /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3  (str)
replay:                     uniform                                       (str)
replay_size:                1000000.0                                     (float)
replay_online:              False                                         (bool)
eval_dir:                                                                 (str)
filter:                     .*                                            (str)
jax.platform:               gpu                                           (str)
jax.jit:                    True                                          (bool)
jax.precision:              float16                                       (str)
jax.prealloc:               True                                          (bool)
jax.debug_nans:             False                                         (bool)
jax.logical_cpus:           0                                             (int)
jax.debug:                  False                                         (bool)
jax.policy_devices:         [0]                                           (ints)
jax.train_devices:          [0]                                           (ints)
jax.metrics_every:          10                                            (int)
run.script:                 train_eval                                    (str)
run.steps:                  10000000000.0                                 (float)
run.expl_until:             0                                             (int)
run.log_every:              300                                           (int)
run.save_every:             900                                           (int)
run.eval_every:             1000000.0                                     (float)
run.eval_initial:           True                                          (bool)
run.eval_eps:               1                                             (int)
run.eval_samples:           1                                             (int)
run.train_ratio:            512.0                                         (float)
run.train_fill:             0                                             (int)
run.eval_fill:              0                                             (int)
run.log_zeros:              False                                         (bool)
run.log_keys_video:         [image]                                       (strs)
run.log_keys_sum:           ^log_reward$                                  (str)
run.log_keys_mean:          (log_entropy)                                 (str)
run.log_keys_max:           ^log_achievement_.*                           (str)
run.from_checkpoint:                                                      (str)
run.sync_every:             10                                            (int)
run.actor_addr:             ipc:///tmp/5551                               (str)
run.actor_batch:            32                                            (int)
envs.amount:                1                                             (int)
envs.parallel:              process                                       (str)
envs.length:                0                                             (int)
envs.reset:                 True                                          (bool)
envs.restart:               True                                          (bool)
envs.discretize:            0                                             (int)
envs.checks:                False                                         (bool)
wrapper.length:             0                                             (int)
wrapper.reset:              True                                          (bool)
wrapper.discretize:         0                                             (int)
wrapper.checks:             False                                         (bool)
env.atari.size:             [64, 64]                                      (ints)
env.atari.repeat:           4                                             (int)
env.atari.sticky:           True                                          (bool)
env.atari.gray:             False                                         (bool)
env.atari.actions:          all                                           (str)
env.atari.lives:            unused                                        (str)
env.atari.noops:            0                                             (int)
env.atari.resize:           opencv                                        (str)
env.dmlab.size:             [64, 64]                                      (ints)
env.dmlab.repeat:           4                                             (int)
env.dmlab.episodic:         True                                          (bool)
env.minecraft.size:         [64, 64]                                      (ints)
env.minecraft.break_speed:  100.0                                         (float)
env.dmc.size:               [64, 64]                                      (ints)
env.dmc.repeat:             2                                             (int)
env.dmc.camera:             -1                                            (int)
env.loconav.size:           [64, 64]                                      (ints)
env.loconav.repeat:         2                                             (int)
env.loconav.camera:         -1                                            (int)
task_behavior:              Greedy                                        (str)
expl_behavior:              None                                          (str)
batch_size:                 16                                            (int)
batch_length:               64                                            (int)
data_loaders:               8                                             (int)
grad_heads:                 [decoder, reward, cont]                       (strs)
rssm.deter:                 4096                                          (int)
rssm.units:                 1024                                          (int)
rssm.stoch:                 32                                            (int)
rssm.classes:               32                                            (int)
rssm.act:                   silu                                          (str)
rssm.norm:                  layer                                         (str)
rssm.initial:               learned                                       (str)
rssm.unimix:                0.01                                          (float)
rssm.unroll:                False                                         (bool)
rssm.action_clip:           1.0                                           (float)
rssm.winit:                 normal                                        (str)
rssm.fan:                   avg                                           (str)
encoder.mlp_keys:           transition_tokens                             (str)
encoder.cnn_keys:           image                                         (str)
encoder.act:                silu                                          (str)
encoder.norm:               layer                                         (str)
encoder.mlp_layers:         5                                             (int)
encoder.mlp_units:          1024                                          (int)
encoder.cnn:                resnet                                        (str)
encoder.cnn_depth:          96                                            (int)
encoder.cnn_blocks:         0                                             (int)
encoder.resize:             stride                                        (str)
encoder.winit:              normal                                        (str)
encoder.fan:                avg                                           (str)
encoder.symlog_inputs:      True                                          (bool)
encoder.minres:             4                                             (int)
decoder.mlp_keys:           transition_tokens                             (str)
decoder.cnn_keys:           image                                         (str)
decoder.act:                silu                                          (str)
decoder.norm:               layer                                         (str)
decoder.mlp_layers:         5                                             (int)
decoder.mlp_units:          1024                                          (int)
decoder.cnn:                resnet                                        (str)
decoder.cnn_depth:          96                                            (int)
decoder.cnn_blocks:         0                                             (int)
decoder.image_dist:         mse                                           (str)
decoder.vector_dist:        symlog_mse                                    (str)
decoder.inputs:             [deter, stoch]                                (strs)
decoder.resize:             stride                                        (str)
decoder.winit:              normal                                        (str)
decoder.fan:                avg                                           (str)
decoder.outscale:           1.0                                           (float)
decoder.minres:             4                                             (int)
decoder.cnn_sigmoid:        False                                         (bool)
reward_head.layers:         5                                             (int)
reward_head.units:          1024                                          (int)
reward_head.act:            silu                                          (str)
reward_head.norm:           layer                                         (str)
reward_head.dist:           symlog_disc                                   (str)
reward_head.outscale:       0.0                                           (float)
reward_head.outnorm:        False                                         (bool)
reward_head.inputs:         [deter, stoch]                                (strs)
reward_head.winit:          normal                                        (str)
reward_head.fan:            avg                                           (str)
reward_head.bins:           255                                           (int)
cont_head.layers:           5                                             (int)
cont_head.units:            1024                                          (int)
cont_head.act:              silu                                          (str)
cont_head.norm:             layer                                         (str)
cont_head.dist:             binary                                        (str)
cont_head.outscale:         1.0                                           (float)
cont_head.outnorm:          False                                         (bool)
cont_head.inputs:           [deter, stoch]                                (strs)
cont_head.winit:            normal                                        (str)
cont_head.fan:              avg                                           (str)
loss_scales.image:          1.0                                           (float)
loss_scales.vector:         1.0                                           (float)
loss_scales.reward:         1.0                                           (float)
loss_scales.cont:           1.0                                           (float)
loss_scales.dyn:            0.5                                           (float)
loss_scales.rep:            0.1                                           (float)
loss_scales.actor:          1.0                                           (float)
loss_scales.critic:         1.0                                           (float)
loss_scales.slowreg:        1.0                                           (float)
dyn_loss.impl:              kl                                            (str)
dyn_loss.free:              1.0                                           (float)
rep_loss.impl:              kl                                            (str)
rep_loss.free:              1.0                                           (float)
model_opt.opt:              adam                                          (str)
model_opt.lr:               0.0001                                        (float)
model_opt.eps:              1e-08                                         (float)
model_opt.clip:             1000.0                                        (float)
model_opt.wd:               0.0                                           (float)
model_opt.warmup:           0                                             (int)
model_opt.lateclip:         0.0                                           (float)
actor.layers:               5                                             (int)
actor.units:                1024                                          (int)
actor.act:                  silu                                          (str)
actor.norm:                 layer                                         (str)
actor.minstd:               0.1                                           (float)
actor.maxstd:               1.0                                           (float)
actor.outscale:             1.0                                           (float)
actor.outnorm:              False                                         (bool)
actor.unimix:               0.01                                          (float)
actor.inputs:               [deter, stoch]                                (strs)
actor.winit:                normal                                        (str)
actor.fan:                  avg                                           (str)
actor.symlog_inputs:        False                                         (bool)
critic.layers:              5                                             (int)
critic.units:               1024                                          (int)
critic.act:                 silu                                          (str)
critic.norm:                layer                                         (str)
critic.dist:                symlog_disc                                   (str)
critic.outscale:            0.0                                           (float)
critic.outnorm:             False                                         (bool)
critic.inputs:              [deter, stoch]                                (strs)
critic.winit:               normal                                        (str)
critic.fan:                 avg                                           (str)
critic.bins:                255                                           (int)
critic.symlog_inputs:       False                                         (bool)
actor_opt.opt:              adam                                          (str)
actor_opt.lr:               3e-05                                         (float)
actor_opt.eps:              1e-05                                         (float)
actor_opt.clip:             100.0                                         (float)
actor_opt.wd:               0.0                                           (float)
actor_opt.warmup:           0                                             (int)
actor_opt.lateclip:         0.0                                           (float)
critic_opt.opt:             adam                                          (str)
critic_opt.lr:              3e-05                                         (float)
critic_opt.eps:             1e-05                                         (float)
critic_opt.clip:            100.0                                         (float)
critic_opt.wd:              0.0                                           (float)
critic_opt.warmup:          0                                             (int)
critic_opt.lateclip:        0.0                                           (float)
actor_dist_disc:            onehot                                        (str)
actor_dist_cont:            normal                                        (str)
actor_grad_disc:            reinforce                                     (str)
actor_grad_cont:            backprop                                      (str)
critic_type:                vfunction                                     (str)
imag_horizon:               15                                            (int)
imag_unroll:                False                                         (bool)
horizon:                    333                                           (int)
return_lambda:              0.95                                          (float)
critic_slowreg:             logprob                                       (str)
slow_critic_update:         1                                             (int)
slow_critic_fraction:       0.02                                          (float)
retnorm.impl:               perc_ema                                      (str)
retnorm.decay:              0.99                                          (float)
retnorm.max:                1.0                                           (float)
retnorm.perclo:             5.0                                           (float)
retnorm.perchi:             95.0                                          (float)
actent:                     0.0003                                        (float)
expl_rewards.extr:          1.0                                           (float)
expl_rewards.disag:         0.1                                           (float)
expl_opt.opt:               adam                                          (str)
expl_opt.lr:                0.0001                                        (float)
expl_opt.eps:               1e-05                                         (float)
expl_opt.clip:              100.0                                         (float)
expl_opt.wd:                0.0                                           (float)
expl_opt.warmup:            0                                             (int)
disag_head.layers:          5                                             (int)
disag_head.units:           1024                                          (int)
disag_head.act:             silu                                          (str)
disag_head.norm:            layer                                         (str)
disag_head.dist:            mse                                           (str)
disag_head.outscale:        1.0                                           (float)
disag_head.inputs:          [deter, stoch, action]                        (strs)
disag_head.winit:           normal                                        (str)
disag_head.fan:             avg                                           (str)
disag_target:               [stoch]                                       (strs)
disag_models:               8                                             (int) 
remaining: []

Config:
use_wandb:                  True                                          (bool)
seed:                       1                                             (int)
method:                     name                                          (str)
task:                       crafter_reward                                (str)
logdir:                     /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3  (str)
replay:                     uniform                                       (str)
replay_size:                1000000.0                                     (float)
replay_online:              False                                         (bool)
eval_dir:                                                                 (str)
filter:                     .*                                            (str)
jax.platform:               gpu                                           (str)
jax.jit:                    True                                          (bool)
jax.precision:              float16                                       (str)
jax.prealloc:               True                                          (bool)
jax.debug_nans:             False                                         (bool)
jax.logical_cpus:           0                                             (int)
jax.debug:                  False                                         (bool)
jax.policy_devices:         [0]                                           (ints)
jax.train_devices:          [0]                                           (ints)
jax.metrics_every:          10                                            (int)
run.script:                 train_eval                                    (str)
run.steps:                  10000000000.0                                 (float)
run.expl_until:             0                                             (int)
run.log_every:              300                                           (int)
run.save_every:             900                                           (int)
run.eval_every:             1000000.0                                     (float)
run.eval_initial:           True                                          (bool)
run.eval_eps:               1                                             (int)
run.eval_samples:           1                                             (int)
run.train_ratio:            512.0                                         (float)
run.train_fill:             0                                             (int)
run.eval_fill:              0                                             (int)
run.log_zeros:              False                                         (bool)
run.log_keys_video:         [image]                                       (strs)
run.log_keys_sum:           ^log_reward$                                  (str)
run.log_keys_mean:          (log_entropy)                                 (str)
run.log_keys_max:           ^log_achievement_.*                           (str)
run.from_checkpoint:                                                      (str)
run.sync_every:             10                                            (int)
run.actor_addr:             ipc:///tmp/5551                               (str)
run.actor_batch:            32                                            (int)
envs.amount:                1                                             (int)
envs.parallel:              process                                       (str)
envs.length:                0                                             (int)
envs.reset:                 True                                          (bool)
envs.restart:               True                                          (bool)
envs.discretize:            0                                             (int)
envs.checks:                False                                         (bool)
wrapper.length:             0                                             (int)
wrapper.reset:              True                                          (bool)
wrapper.discretize:         0                                             (int)
wrapper.checks:             False                                         (bool)
env.atari.size:             [64, 64]                                      (ints)
env.atari.repeat:           4                                             (int)
env.atari.sticky:           True                                          (bool)
env.atari.gray:             False                                         (bool)
env.atari.actions:          all                                           (str)
env.atari.lives:            unused                                        (str)
env.atari.noops:            0                                             (int)
env.atari.resize:           opencv                                        (str)
env.dmlab.size:             [64, 64]                                      (ints)
env.dmlab.repeat:           4                                             (int)
env.dmlab.episodic:         True                                          (bool)
env.minecraft.size:         [64, 64]                                      (ints)
env.minecraft.break_speed:  100.0                                         (float)
env.dmc.size:               [64, 64]                                      (ints)
env.dmc.repeat:             2                                             (int)
env.dmc.camera:             -1                                            (int)
env.loconav.size:           [64, 64]                                      (ints)
env.loconav.repeat:         2                                             (int)
env.loconav.camera:         -1                                            (int)
task_behavior:              Greedy                                        (str)
expl_behavior:              None                                          (str)
batch_size:                 16                                            (int)
batch_length:               64                                            (int)
data_loaders:               8                                             (int)
grad_heads:                 [decoder, reward, cont]                       (strs)
rssm.deter:                 4096                                          (int)
rssm.units:                 1024                                          (int)
rssm.stoch:                 32                                            (int)
rssm.classes:               32                                            (int)
rssm.act:                   silu                                          (str)
rssm.norm:                  layer                                         (str)
rssm.initial:               learned                                       (str)
rssm.unimix:                0.01                                          (float)
rssm.unroll:                False                                         (bool)
rssm.action_clip:           1.0                                           (float)
rssm.winit:                 normal                                        (str)
rssm.fan:                   avg                                           (str)
encoder.mlp_keys:           transition_tokens                             (str)
encoder.cnn_keys:           image                                         (str)
encoder.act:                silu                                          (str)
encoder.norm:               layer                                         (str)
encoder.mlp_layers:         5                                             (int)
encoder.mlp_units:          1024                                          (int)
encoder.cnn:                resnet                                        (str)
encoder.cnn_depth:          96                                            (int)
encoder.cnn_blocks:         0                                             (int)
encoder.resize:             stride                                        (str)
encoder.winit:              normal                                        (str)
encoder.fan:                avg                                           (str)
encoder.symlog_inputs:      True                                          (bool)
encoder.minres:             4                                             (int)
decoder.mlp_keys:           transition_tokens                             (str)
decoder.cnn_keys:           image                                         (str)
decoder.act:                silu                                          (str)
decoder.norm:               layer                                         (str)
decoder.mlp_layers:         5                                             (int)
decoder.mlp_units:          1024                                          (int)
decoder.cnn:                resnet                                        (str)
decoder.cnn_depth:          96                                            (int)
decoder.cnn_blocks:         0                                             (int)
decoder.image_dist:         mse                                           (str)
decoder.vector_dist:        symlog_mse                                    (str)
decoder.inputs:             [deter, stoch]                                (strs)
decoder.resize:             stride                                        (str)
decoder.winit:              normal                                        (str)
decoder.fan:                avg                                           (str)
decoder.outscale:           1.0                                           (float)
decoder.minres:             4                                             (int)
decoder.cnn_sigmoid:        False                                         (bool)
reward_head.layers:         5                                             (int)
reward_head.units:          1024                                          (int)
reward_head.act:            silu                                          (str)
reward_head.norm:           layer                                         (str)
reward_head.dist:           symlog_disc                                   (str)
reward_head.outscale:       0.0                                           (float)
reward_head.outnorm:        False                                         (bool)
reward_head.inputs:         [deter, stoch]                                (strs)
reward_head.winit:          normal                                        (str)
reward_head.fan:            avg                                           (str)
reward_head.bins:           255                                           (int)
cont_head.layers:           5                                             (int)
cont_head.units:            1024                                          (int)
cont_head.act:              silu                                          (str)
cont_head.norm:             layer                                         (str)
cont_head.dist:             binary                                        (str)
cont_head.outscale:         1.0                                           (float)
cont_head.outnorm:          False                                         (bool)
cont_head.inputs:           [deter, stoch]                                (strs)
cont_head.winit:            normal                                        (str)
cont_head.fan:              avg                                           (str)
loss_scales.image:          1.0                                           (float)
loss_scales.vector:         1.0                                           (float)
loss_scales.reward:         1.0                                           (float)
loss_scales.cont:           1.0                                           (float)
loss_scales.dyn:            0.5                                           (float)
loss_scales.rep:            0.1                                           (float)
loss_scales.actor:          1.0                                           (float)
loss_scales.critic:         1.0                                           (float)
loss_scales.slowreg:        1.0                                           (float)
dyn_loss.impl:              kl                                            (str)
dyn_loss.free:              1.0                                           (float)
rep_loss.impl:              kl                                            (str)
rep_loss.free:              1.0                                           (float)
model_opt.opt:              adam                                          (str)
model_opt.lr:               0.0001                                        (float)
model_opt.eps:              1e-08                                         (float)
model_opt.clip:             1000.0                                        (float)
model_opt.wd:               0.0                                           (float)
model_opt.warmup:           0                                             (int)
model_opt.lateclip:         0.0                                           (float)
actor.layers:               5                                             (int)
actor.units:                1024                                          (int)
actor.act:                  silu                                          (str)
actor.norm:                 layer                                         (str)
actor.minstd:               0.1                                           (float)
actor.maxstd:               1.0                                           (float)
actor.outscale:             1.0                                           (float)
actor.outnorm:              False                                         (bool)
actor.unimix:               0.01                                          (float)
actor.inputs:               [deter, stoch]                                (strs)
actor.winit:                normal                                        (str)
actor.fan:                  avg                                           (str)
actor.symlog_inputs:        False                                         (bool)
critic.layers:              5                                             (int)
critic.units:               1024                                          (int)
critic.act:                 silu                                          (str)
critic.norm:                layer                                         (str)
critic.dist:                symlog_disc                                   (str)
critic.outscale:            0.0                                           (float)
critic.outnorm:             False                                         (bool)
critic.inputs:              [deter, stoch]                                (strs)
critic.winit:               normal                                        (str)
critic.fan:                 avg                                           (str)
critic.bins:                255                                           (int)
critic.symlog_inputs:       False                                         (bool)
actor_opt.opt:              adam                                          (str)
actor_opt.lr:               3e-05                                         (float)
actor_opt.eps:              1e-05                                         (float)
actor_opt.clip:             100.0                                         (float)
actor_opt.wd:               0.0                                           (float)
actor_opt.warmup:           0                                             (int)
actor_opt.lateclip:         0.0                                           (float)
critic_opt.opt:             adam                                          (str)
critic_opt.lr:              3e-05                                         (float)
critic_opt.eps:             1e-05                                         (float)
critic_opt.clip:            100.0                                         (float)
critic_opt.wd:              0.0                                           (float)
critic_opt.warmup:          0                                             (int)
critic_opt.lateclip:        0.0                                           (float)
actor_dist_disc:            onehot                                        (str)
actor_dist_cont:            normal                                        (str)
actor_grad_disc:            reinforce                                     (str)
actor_grad_cont:            backprop                                      (str)
critic_type:                vfunction                                     (str)
imag_horizon:               15                                            (int)
imag_unroll:                False                                         (bool)
horizon:                    333                                           (int)
return_lambda:              0.95                                          (float)
critic_slowreg:             logprob                                       (str)
slow_critic_update:         1                                             (int)
slow_critic_fraction:       0.02                                          (float)
retnorm.impl:               perc_ema                                      (str)
retnorm.decay:              0.99                                          (float)
retnorm.max:                1.0                                           (float)
retnorm.perclo:             5.0                                           (float)
retnorm.perchi:             95.0                                          (float)
actent:                     0.0003                                        (float)
expl_rewards.extr:          1.0                                           (float)
expl_rewards.disag:         0.1                                           (float)
expl_opt.opt:               adam                                          (str)
expl_opt.lr:                0.0001                                        (float)
expl_opt.eps:               1e-05                                         (float)
expl_opt.clip:              100.0                                         (float)
expl_opt.wd:                0.0                                           (float)
expl_opt.warmup:            0                                             (int)
disag_head.layers:          5                                             (int)
disag_head.units:           1024                                          (int)
disag_head.act:             silu                                          (str)
disag_head.norm:            layer                                         (str)
disag_head.dist:            mse                                           (str)
disag_head.outscale:        1.0                                           (float)
disag_head.inputs:          [deter, stoch, action]                        (strs)
disag_head.winit:           normal                                        (str)
disag_head.fan:             avg                                           (str)
disag_target:               [stoch]                                       (strs)
disag_models:               8                                             (int)wandb: Currently logged in as: ibisbill326 (ibisbill326-gmail-com). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/wandb/run-20240102_174011-a4qs02g3
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run ziyu_crafter_cuda_3_seed_3
wandb: ⭐️ View project at https://wandb.ai/ibisbill326-gmail-com/crafter_reward
wandb: 🚀 View run at https://wandb.ai/ibisbill326-gmail-com/crafter_reward/runs/a4qs02g3

!! Resuming wandb run !!
Encoder CNN shapes: {'image': (64, 64, 3)}
Encoder MLP shapes: {'transition_tokens': (384,)}
Decoder CNN shapes: {'image': (64, 64, 3)}
Decoder MLP shapes: {'transition_tokens': (384,)}
JAX devices (1): [cuda(id=0)]
Policy devices: cuda:0
Train devices:  cuda:0
Tracing train function.
no rnd data in data
Optimizer model_opt has 197,057,283 variables.
Optimizer actor_opt has 9,464,849 variables.
Optimizer critic_opt has 9,708,799 variables.
Logdir /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3
Observation space:
  image            Space(dtype=uint8, shape=(64, 64, 3), low=0, high=255)
  transition_tokens Space(dtype=uint32, shape=(384,), low=0, high=4294967295)
  goal_tokens      Space(dtype=uint32, shape=(5, 384), low=0, high=4294967295)
  goal_id          Space(dtype=uint32, shape=(5,), low=0, high=4294967295)
  reward           Space(dtype=float32, shape=(), low=-inf, high=inf)
  is_first         Space(dtype=bool, shape=(), low=False, high=True)
  is_last          Space(dtype=bool, shape=(), low=False, high=True)
  is_terminal      Space(dtype=bool, shape=(), low=False, high=True)
  log_reward       Space(dtype=float32, shape=(1,), low=-inf, high=inf)
  log_achievement_collect_coal Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_diamond Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_drink Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_iron Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_sapling Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_wood Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_skeleton Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_zombie Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_cow Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_furnace Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_table Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_wake_up Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
Action space:
  action           Space(dtype=float32, shape=(17,), low=0, high=1)
  reset            Space(dtype=bool, shape=(), low=False, high=True)
Prefill train dataset.
Prefill eval dataset.
Found existing checkpoint.
Loading checkpoint: /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3/checkpoint.ckpt
Loaded checkpoint from 38198 seconds ago.
Start training loop.
Starting evaluation at step 19600
Tracing policy function.
Tracing policy function.
Episode has 39 steps and return 1.1.
Tracing policy function.
Tracing train function.
Tracing report function.
Tracing report function.
Tracing report function.
────────────────────────────────── Step 19601 ──────────────────────────────────
eval_episode/length 39 / eval_episode/score 1.1 / eval_episode/reward_rate 0.97 
/ eval_stats/sum_log_reward 1.1 / eval_stats/max_log_achievement_collect_sapling
1 / eval_stats/max_log_achievement_place_plant 1 / train/action_mag 16 / 
train/action_max 16 / train/action_mean 6.76 / train/action_min 0 / 
train/action_std 4.89 / train/actor_opt_actor_opt_grad_overflow 0 / 
train/actor_opt_actor_opt_grad_scale 1e4 / train/actor_opt_grad_norm 0.04 / 
train/actor_opt_grad_steps 9701 / train/actor_opt_loss -6.73 / train/adv_mag 
0.52 / train/adv_max 0.52 / train/adv_mean 1.7e-3 / train/adv_min -0.42 / 
train/adv_std 0.06 / train/cont_avg 0.99 / train/cont_loss_mean 1.2e-3 / 
train/cont_loss_std 0.03 / train/cont_neg_acc 0.89 / train/cont_neg_loss 0.13 / 
train/cont_pos_acc 1 / train/cont_pos_loss 1.1e-4 / train/cont_pred 0.99 / 
train/cont_rate 0.99 / train/dyn_loss_mean 6.44 / train/dyn_loss_std 15 / 
train/extr_critic_critic_opt_critic_opt_grad_overflow 0 / 
train/extr_critic_critic_opt_critic_opt_grad_scale 1e4 / 
train/extr_critic_critic_opt_grad_norm 2.01 / 
train/extr_critic_critic_opt_grad_steps 9701 / train/extr_critic_critic_opt_loss
1.7e4 / train/extr_critic_mag 8.73 / train/extr_critic_max 8.73 / 
train/extr_critic_mean 1.89 / train/extr_critic_min -0.53 / 
train/extr_critic_std 2.08 / train/extr_return_normed_mag 1.59 / 
train/extr_return_normed_max 1.59 / train/extr_return_normed_mean 0.34 / 
train/extr_return_normed_min -0.11 / train/extr_return_normed_std 0.34 / 
train/extr_return_rate 0.62 / train/extr_return_raw_mag 9.69 / 
train/extr_return_raw_max 9.69 / train/extr_return_raw_mean 1.9 / 
train/extr_return_raw_min -0.92 / train/extr_return_raw_std 2.11 / 
train/extr_reward_mag 1 / train/extr_reward_max 1 / train/extr_reward_mean 0.02 
/ train/extr_reward_min -0.7 / train/extr_reward_std 0.16 / 
train/image_loss_mean 16.29 / train/image_loss_std 61.78 / train/model_loss_mean
20.22 / train/model_loss_std 68.71 / train/model_opt_grad_norm 71.26 / 
train/model_opt_grad_steps 9688 / train/model_opt_loss 1.3e4 / 
train/model_opt_model_opt_grad_overflow 0 / train/model_opt_model_opt_grad_scale
625 / train/policy_entropy_mag 2.42 / train/policy_entropy_max 2.42 / 
train/policy_entropy_mean 0.67 / train/policy_entropy_min 0.08 / 
train/policy_entropy_std 0.56 / train/policy_logprob_mag 7.44 / 
train/policy_logprob_max -9.5e-3 / train/policy_logprob_mean -0.67 / 
train/policy_logprob_min -7.44 / train/policy_logprob_std 1.18 / 
train/policy_randomness_mag 0.85 / train/policy_randomness_max 0.85 / 
train/policy_randomness_mean 0.24 / train/policy_randomness_min 0.03 / 
train/policy_randomness_std 0.2 / train/post_ent_mag 39.52 / train/post_ent_max 
39.52 / train/post_ent_mean 22.11 / train/post_ent_min 8.47 / train/post_ent_std
5.02 / train/prior_ent_mag 63.88 / train/prior_ent_max 63.88 / 
train/prior_ent_mean 26.64 / train/prior_ent_min 11.23 / train/prior_ent_std 
9.15 / train/rep_loss_mean 6.44 / train/rep_loss_std 15 / train/reward_avg 0.01 
/ train/reward_loss_mean 0.07 / train/reward_loss_std 0.43 / 
train/reward_max_data 1 / train/reward_max_pred 1 / train/reward_neg_acc 1 / 
train/reward_neg_loss 0.05 / train/reward_pos_acc 0.95 / train/reward_pos_loss 
0.87 / train/reward_pred 0.01 / train/reward_rate 0.02 / 
train/transition_tokens_loss_mean 1.1e-3 / train/transition_tokens_loss_std 
3.6e-5 / train/params_agent/wm/model_opt 2e8 / 
train/params_agent/task_behavior/critic/critic_opt 9.7e6 / 
train/params_agent/task_behavior/ac/actor_opt 9.5e6 / report/cont_avg 0.99 / 
report/cont_loss_mean 9.5e-4 / report/cont_loss_std 0.02 / report/cont_neg_acc 1
/ report/cont_neg_loss 0.09 / report/cont_pos_acc 1 / report/cont_pos_loss 
1.3e-4 / report/cont_pred 0.99 / report/cont_rate 0.99 / report/dyn_loss_mean 
5.38 / report/dyn_loss_std 11.61 / report/image_loss_mean 10.22 / 
report/image_loss_std 35.82 / report/model_loss_mean 13.51 / 
report/model_loss_std 40.97 / report/post_ent_mag 41.85 / report/post_ent_max 
41.85 / report/post_ent_mean 22.3 / report/post_ent_min 8.75 / 
report/post_ent_std 4.94 / report/prior_ent_mag 63.83 / report/prior_ent_max 
63.83 / report/prior_ent_mean 26.56 / report/prior_ent_min 11.71 / 
report/prior_ent_std 9.15 / report/rep_loss_mean 5.38 / report/rep_loss_std 
11.61 / report/reward_avg 0.01 / report/reward_loss_mean 0.06 / 
report/reward_loss_std 0.35 / report/reward_max_data 1 / report/reward_max_pred 
1 / report/reward_neg_acc 1 / report/reward_neg_loss 0.04 / 
report/reward_pos_acc 0.95 / report/reward_pos_loss 0.9 / report/reward_pred 
0.01 / report/reward_rate 0.02 / report/transition_tokens_loss_mean 1e-3 / 
report/transition_tokens_loss_std 3.5e-5 / eval/cont_avg 0.99 / 
eval/cont_loss_mean 3.8e-3 / eval/cont_loss_std 0.07 / eval/cont_neg_acc 1 / 
eval/cont_neg_loss 0.14 / eval/cont_pos_acc 1 / eval/cont_pos_loss 3e-3 / 
eval/cont_pred 0.99 / eval/cont_rate 0.99 / eval/dyn_loss_mean 24.27 / 
eval/dyn_loss_std 13.53 / eval/image_loss_mean 51.6 / eval/image_loss_std 47.89 
/ eval/model_loss_mean 66.35 / eval/model_loss_std 52.45 / eval/post_ent_mag 
38.45 / eval/post_ent_max 38.45 / eval/post_ent_mean 25.69 / eval/post_ent_min 
10.79 / eval/post_ent_std 4.91 / eval/prior_ent_mag 63.83 / eval/prior_ent_max 
63.83 / eval/prior_ent_mean 31.97 / eval/prior_ent_min 10.81 / 
eval/prior_ent_std 8.62 / eval/rep_loss_mean 24.27 / eval/rep_loss_std 13.53 / 
eval/reward_avg 5.4e-3 / eval/reward_loss_mean 0.18 / eval/reward_loss_std 0.95 
/ eval/reward_max_data 1 / eval/reward_max_pred 1.01 / eval/reward_neg_acc 1 / 
eval/reward_neg_loss 0.14 / eval/reward_pos_acc 0.45 / eval/reward_pos_loss 4.58
/ eval/reward_pred 2.1e-3 / eval/reward_rate 0.01 / 
eval/transition_tokens_loss_mean 1e-3 / eval/transition_tokens_loss_std 3.3e-5 /
replay/size 5.5e4 / replay/inserts 0 / replay/samples 112 / 
replay/insert_wait_avg nan / replay/insert_wait_frac nan / 
replay/sample_wait_avg 1.8e-6 / replay/sample_wait_frac 1 / eval_replay/size 
2692 / eval_replay/inserts 0 / eval_replay/samples 112 / 
eval_replay/insert_wait_avg nan / eval_replay/insert_wait_frac nan / 
eval_replay/sample_wait_avg 2e-6 / eval_replay/sample_wait_frac 1 / 
timer/duration 146.28 / timer/replay._sample_count 112 / 
timer/replay._sample_total 20.11 / timer/replay._sample_frac 0.14 / 
timer/replay._sample_avg 0.18 / timer/replay._sample_min 0.05 / 
timer/replay._sample_max 1.25 / timer/agent.policy_count 41 / 
timer/agent.policy_total 7.02 / timer/agent.policy_frac 0.05 / 
timer/agent.policy_avg 0.17 / timer/agent.policy_min 8.7e-3 / 
timer/agent.policy_max 4.82 / timer/env.step_count 1 / timer/env.step_total 1.28
/ timer/env.step_frac 8.8e-3 / timer/env.step_avg 1.28 / timer/env.step_min 1.28
/ timer/env.step_max 1.28 / timer/dataset_train_count 1 / 
timer/dataset_train_total 6.1e-5 / timer/dataset_train_frac 4.2e-7 / 
timer/dataset_train_avg 6.1e-5 / timer/dataset_train_min 6.1e-5 / 
timer/dataset_train_max 6.1e-5 / timer/agent.train_count 1 / 
timer/agent.train_total 100.62 / timer/agent.train_frac 0.69 / 
timer/agent.train_avg 100.62 / timer/agent.train_min 100.62 / 
timer/agent.train_max 100.62 / timer/agent.report_count 2 / 
timer/agent.report_total 22.37 / timer/agent.report_frac 0.15 / 
timer/agent.report_avg 11.18 / timer/agent.report_min 5.93 / 
timer/agent.report_max 16.44 / timer/dataset_eval_count 1 / 
timer/dataset_eval_total 1.3e-4 / timer/dataset_eval_frac 9.1e-7 / 
timer/dataset_eval_avg 1.3e-4 / timer/dataset_eval_min 1.3e-4 / 
timer/dataset_eval_max 1.3e-4
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/ziyu/logdir/ziyu_crafter_cuda_3_seed_3
parsed:  
Config:
configs:  [crafter]  (strs) 
remaining: ['--run.script', 'train_eval', '--use_wandb', 'True', '--logdir', '/home/ziyu/logdir/ziyu_crafter_cuda_0_seed_0']
parsed:  
Config:
use_wandb:                  True                                          (bool)
seed:                       1                                             (int)
method:                     name                                          (str)
task:                       crafter_reward                                (str)
logdir:                     /home/ziyu/logdir/ziyu_crafter_cuda_0_seed_0  (str)
replay:                     uniform                                       (str)
replay_size:                1000000.0                                     (float)
replay_online:              False                                         (bool)
eval_dir:                                                                 (str)
filter:                     .*                                            (str)
jax.platform:               gpu                                           (str)
jax.jit:                    True                                          (bool)
jax.precision:              float16                                       (str)
jax.prealloc:               True                                          (bool)
jax.debug_nans:             False                                         (bool)
jax.logical_cpus:           0                                             (int)
jax.debug:                  False                                         (bool)
jax.policy_devices:         [0]                                           (ints)
jax.train_devices:          [0]                                           (ints)
jax.metrics_every:          10                                            (int)
run.script:                 train_eval                                    (str)
run.steps:                  10000000000.0                                 (float)
run.expl_until:             0                                             (int)
run.log_every:              300                                           (int)
run.save_every:             900                                           (int)
run.eval_every:             1000000.0                                     (float)
run.eval_initial:           True                                          (bool)
run.eval_eps:               1                                             (int)
run.eval_samples:           1                                             (int)
run.train_ratio:            512.0                                         (float)
run.train_fill:             0                                             (int)
run.eval_fill:              0                                             (int)
run.log_zeros:              False                                         (bool)
run.log_keys_video:         [image]                                       (strs)
run.log_keys_sum:           ^log_reward$                                  (str)
run.log_keys_mean:          (log_entropy)                                 (str)
run.log_keys_max:           ^log_achievement_.*                           (str)
run.from_checkpoint:                                                      (str)
run.sync_every:             10                                            (int)
run.actor_addr:             ipc:///tmp/5551                               (str)
run.actor_batch:            32                                            (int)
envs.amount:                1                                             (int)
envs.parallel:              process                                       (str)
envs.length:                0                                             (int)
envs.reset:                 True                                          (bool)
envs.restart:               True                                          (bool)
envs.discretize:            0                                             (int)
envs.checks:                False                                         (bool)
wrapper.length:             0                                             (int)
wrapper.reset:              True                                          (bool)
wrapper.discretize:         0                                             (int)
wrapper.checks:             False                                         (bool)
env.atari.size:             [64, 64]                                      (ints)
env.atari.repeat:           4                                             (int)
env.atari.sticky:           True                                          (bool)
env.atari.gray:             False                                         (bool)
env.atari.actions:          all                                           (str)
env.atari.lives:            unused                                        (str)
env.atari.noops:            0                                             (int)
env.atari.resize:           opencv                                        (str)
env.dmlab.size:             [64, 64]                                      (ints)
env.dmlab.repeat:           4                                             (int)
env.dmlab.episodic:         True                                          (bool)
env.minecraft.size:         [64, 64]                                      (ints)
env.minecraft.break_speed:  100.0                                         (float)
env.dmc.size:               [64, 64]                                      (ints)
env.dmc.repeat:             2                                             (int)
env.dmc.camera:             -1                                            (int)
env.loconav.size:           [64, 64]                                      (ints)
env.loconav.repeat:         2                                             (int)
env.loconav.camera:         -1                                            (int)
task_behavior:              Greedy                                        (str)
expl_behavior:              None                                          (str)
batch_size:                 16                                            (int)
batch_length:               64                                            (int)
data_loaders:               8                                             (int)
grad_heads:                 [decoder, reward, cont]                       (strs)
rssm.deter:                 4096                                          (int)
rssm.units:                 1024                                          (int)
rssm.stoch:                 32                                            (int)
rssm.classes:               32                                            (int)
rssm.act:                   silu                                          (str)
rssm.norm:                  layer                                         (str)
rssm.initial:               learned                                       (str)
rssm.unimix:                0.01                                          (float)
rssm.unroll:                False                                         (bool)
rssm.action_clip:           1.0                                           (float)
rssm.winit:                 normal                                        (str)
rssm.fan:                   avg                                           (str)
encoder.mlp_keys:           transition_tokens                             (str)
encoder.cnn_keys:           image                                         (str)
encoder.act:                silu                                          (str)
encoder.norm:               layer                                         (str)
encoder.mlp_layers:         5                                             (int)
encoder.mlp_units:          1024                                          (int)
encoder.cnn:                resnet                                        (str)
encoder.cnn_depth:          96                                            (int)
encoder.cnn_blocks:         0                                             (int)
encoder.resize:             stride                                        (str)
encoder.winit:              normal                                        (str)
encoder.fan:                avg                                           (str)
encoder.symlog_inputs:      True                                          (bool)
encoder.minres:             4                                             (int)
decoder.mlp_keys:           transition_tokens                             (str)
decoder.cnn_keys:           image                                         (str)
decoder.act:                silu                                          (str)
decoder.norm:               layer                                         (str)
decoder.mlp_layers:         5                                             (int)
decoder.mlp_units:          1024                                          (int)
decoder.cnn:                resnet                                        (str)
decoder.cnn_depth:          96                                            (int)
decoder.cnn_blocks:         0                                             (int)
decoder.image_dist:         mse                                           (str)
decoder.vector_dist:        symlog_mse                                    (str)
decoder.inputs:             [deter, stoch]                                (strs)
decoder.resize:             stride                                        (str)
decoder.winit:              normal                                        (str)
decoder.fan:                avg                                           (str)
decoder.outscale:           1.0                                           (float)
decoder.minres:             4                                             (int)
decoder.cnn_sigmoid:        False                                         (bool)
reward_head.layers:         5                                             (int)
reward_head.units:          1024                                          (int)
reward_head.act:            silu                                          (str)
reward_head.norm:           layer                                         (str)
reward_head.dist:           symlog_disc                                   (str)
reward_head.outscale:       0.0                                           (float)
reward_head.outnorm:        False                                         (bool)
reward_head.inputs:         [deter, stoch]                                (strs)
reward_head.winit:          normal                                        (str)
reward_head.fan:            avg                                           (str)
reward_head.bins:           255                                           (int)
cont_head.layers:           5                                             (int)
cont_head.units:            1024                                          (int)
cont_head.act:              silu                                          (str)
cont_head.norm:             layer                                         (str)
cont_head.dist:             binary                                        (str)
cont_head.outscale:         1.0                                           (float)
cont_head.outnorm:          False                                         (bool)
cont_head.inputs:           [deter, stoch]                                (strs)
cont_head.winit:            normal                                        (str)
cont_head.fan:              avg                                           (str)
loss_scales.image:          1.0                                           (float)
loss_scales.vector:         1.0                                           (float)
loss_scales.reward:         1.0                                           (float)
loss_scales.cont:           1.0                                           (float)
loss_scales.dyn:            0.5                                           (float)
loss_scales.rep:            0.1                                           (float)
loss_scales.actor:          1.0                                           (float)
loss_scales.critic:         1.0                                           (float)
loss_scales.slowreg:        1.0                                           (float)
dyn_loss.impl:              kl                                            (str)
dyn_loss.free:              1.0                                           (float)
rep_loss.impl:              kl                                            (str)
rep_loss.free:              1.0                                           (float)
model_opt.opt:              adam                                          (str)
model_opt.lr:               0.0001                                        (float)
model_opt.eps:              1e-08                                         (float)
model_opt.clip:             1000.0                                        (float)
model_opt.wd:               0.0                                           (float)
model_opt.warmup:           0                                             (int)
model_opt.lateclip:         0.0                                           (float)
actor.layers:               5                                             (int)
actor.units:                1024                                          (int)
actor.act:                  silu                                          (str)
actor.norm:                 layer                                         (str)
actor.minstd:               0.1                                           (float)
actor.maxstd:               1.0                                           (float)
actor.outscale:             1.0                                           (float)
actor.outnorm:              False                                         (bool)
actor.unimix:               0.01                                          (float)
actor.inputs:               [deter, stoch]                                (strs)
actor.winit:                normal                                        (str)
actor.fan:                  avg                                           (str)
actor.symlog_inputs:        False                                         (bool)
critic.layers:              5                                             (int)
critic.units:               1024                                          (int)
critic.act:                 silu                                          (str)
critic.norm:                layer                                         (str)
critic.dist:                symlog_disc                                   (str)
critic.outscale:            0.0                                           (float)
critic.outnorm:             False                                         (bool)
critic.inputs:              [deter, stoch]                                (strs)
critic.winit:               normal                                        (str)
critic.fan:                 avg                                           (str)
critic.bins:                255                                           (int)
critic.symlog_inputs:       False                                         (bool)
actor_opt.opt:              adam                                          (str)
actor_opt.lr:               3e-05                                         (float)
actor_opt.eps:              1e-05                                         (float)
actor_opt.clip:             100.0                                         (float)
actor_opt.wd:               0.0                                           (float)
actor_opt.warmup:           0                                             (int)
actor_opt.lateclip:         0.0                                           (float)
critic_opt.opt:             adam                                          (str)
critic_opt.lr:              3e-05                                         (float)
critic_opt.eps:             1e-05                                         (float)
critic_opt.clip:            100.0                                         (float)
critic_opt.wd:              0.0                                           (float)
critic_opt.warmup:          0                                             (int)
critic_opt.lateclip:        0.0                                           (float)
actor_dist_disc:            onehot                                        (str)
actor_dist_cont:            normal                                        (str)
actor_grad_disc:            reinforce                                     (str)
actor_grad_cont:            backprop                                      (str)
critic_type:                vfunction                                     (str)
imag_horizon:               15                                            (int)
imag_unroll:                False                                         (bool)
horizon:                    333                                           (int)
return_lambda:              0.95                                          (float)
critic_slowreg:             logprob                                       (str)
slow_critic_update:         1                                             (int)
slow_critic_fraction:       0.02                                          (float)
retnorm.impl:               perc_ema                                      (str)
retnorm.decay:              0.99                                          (float)
retnorm.max:                1.0                                           (float)
retnorm.perclo:             5.0                                           (float)
retnorm.perchi:             95.0                                          (float)
actent:                     0.0003                                        (float)
expl_rewards.extr:          1.0                                           (float)
expl_rewards.disag:         0.1                                           (float)
expl_opt.opt:               adam                                          (str)
expl_opt.lr:                0.0001                                        (float)
expl_opt.eps:               1e-05                                         (float)
expl_opt.clip:              100.0                                         (float)
expl_opt.wd:                0.0                                           (float)
expl_opt.warmup:            0                                             (int)
disag_head.layers:          5                                             (int)
disag_head.units:           1024                                          (int)
disag_head.act:             silu                                          (str)
disag_head.norm:            layer                                         (str)
disag_head.dist:            mse                                           (str)
disag_head.outscale:        1.0                                           (float)
disag_head.inputs:          [deter, stoch, action]                        (strs)
disag_head.winit:           normal                                        (str)
disag_head.fan:             avg                                           (str)
disag_target:               [stoch]                                       (strs)
disag_models:               8                                             (int) 
remaining: []

Config:
use_wandb:                  True                                          (bool)
seed:                       1                                             (int)
method:                     name                                          (str)
task:                       crafter_reward                                (str)
logdir:                     /home/ziyu/logdir/ziyu_crafter_cuda_0_seed_0  (str)
replay:                     uniform                                       (str)
replay_size:                1000000.0                                     (float)
replay_online:              False                                         (bool)
eval_dir:                                                                 (str)
filter:                     .*                                            (str)
jax.platform:               gpu                                           (str)
jax.jit:                    True                                          (bool)
jax.precision:              float16                                       (str)
jax.prealloc:               True                                          (bool)
jax.debug_nans:             False                                         (bool)
jax.logical_cpus:           0                                             (int)
jax.debug:                  False                                         (bool)
jax.policy_devices:         [0]                                           (ints)
jax.train_devices:          [0]                                           (ints)
jax.metrics_every:          10                                            (int)
run.script:                 train_eval                                    (str)
run.steps:                  10000000000.0                                 (float)
run.expl_until:             0                                             (int)
run.log_every:              300                                           (int)
run.save_every:             900                                           (int)
run.eval_every:             1000000.0                                     (float)
run.eval_initial:           True                                          (bool)
run.eval_eps:               1                                             (int)
run.eval_samples:           1                                             (int)
run.train_ratio:            512.0                                         (float)
run.train_fill:             0                                             (int)
run.eval_fill:              0                                             (int)
run.log_zeros:              False                                         (bool)
run.log_keys_video:         [image]                                       (strs)
run.log_keys_sum:           ^log_reward$                                  (str)
run.log_keys_mean:          (log_entropy)                                 (str)
run.log_keys_max:           ^log_achievement_.*                           (str)
run.from_checkpoint:                                                      (str)
run.sync_every:             10                                            (int)
run.actor_addr:             ipc:///tmp/5551                               (str)
run.actor_batch:            32                                            (int)
envs.amount:                1                                             (int)
envs.parallel:              process                                       (str)
envs.length:                0                                             (int)
envs.reset:                 True                                          (bool)
envs.restart:               True                                          (bool)
envs.discretize:            0                                             (int)
envs.checks:                False                                         (bool)
wrapper.length:             0                                             (int)
wrapper.reset:              True                                          (bool)
wrapper.discretize:         0                                             (int)
wrapper.checks:             False                                         (bool)
env.atari.size:             [64, 64]                                      (ints)
env.atari.repeat:           4                                             (int)
env.atari.sticky:           True                                          (bool)
env.atari.gray:             False                                         (bool)
env.atari.actions:          all                                           (str)
env.atari.lives:            unused                                        (str)
env.atari.noops:            0                                             (int)
env.atari.resize:           opencv                                        (str)
env.dmlab.size:             [64, 64]                                      (ints)
env.dmlab.repeat:           4                                             (int)
env.dmlab.episodic:         True                                          (bool)
env.minecraft.size:         [64, 64]                                      (ints)
env.minecraft.break_speed:  100.0                                         (float)
env.dmc.size:               [64, 64]                                      (ints)
env.dmc.repeat:             2                                             (int)
env.dmc.camera:             -1                                            (int)
env.loconav.size:           [64, 64]                                      (ints)
env.loconav.repeat:         2                                             (int)
env.loconav.camera:         -1                                            (int)
task_behavior:              Greedy                                        (str)
expl_behavior:              None                                          (str)
batch_size:                 16                                            (int)
batch_length:               64                                            (int)
data_loaders:               8                                             (int)
grad_heads:                 [decoder, reward, cont]                       (strs)
rssm.deter:                 4096                                          (int)
rssm.units:                 1024                                          (int)
rssm.stoch:                 32                                            (int)
rssm.classes:               32                                            (int)
rssm.act:                   silu                                          (str)
rssm.norm:                  layer                                         (str)
rssm.initial:               learned                                       (str)
rssm.unimix:                0.01                                          (float)
rssm.unroll:                False                                         (bool)
rssm.action_clip:           1.0                                           (float)
rssm.winit:                 normal                                        (str)
rssm.fan:                   avg                                           (str)
encoder.mlp_keys:           transition_tokens                             (str)
encoder.cnn_keys:           image                                         (str)
encoder.act:                silu                                          (str)
encoder.norm:               layer                                         (str)
encoder.mlp_layers:         5                                             (int)
encoder.mlp_units:          1024                                          (int)
encoder.cnn:                resnet                                        (str)
encoder.cnn_depth:          96                                            (int)
encoder.cnn_blocks:         0                                             (int)
encoder.resize:             stride                                        (str)
encoder.winit:              normal                                        (str)
encoder.fan:                avg                                           (str)
encoder.symlog_inputs:      True                                          (bool)
encoder.minres:             4                                             (int)
decoder.mlp_keys:           transition_tokens                             (str)
decoder.cnn_keys:           image                                         (str)
decoder.act:                silu                                          (str)
decoder.norm:               layer                                         (str)
decoder.mlp_layers:         5                                             (int)
decoder.mlp_units:          1024                                          (int)
decoder.cnn:                resnet                                        (str)
decoder.cnn_depth:          96                                            (int)
decoder.cnn_blocks:         0                                             (int)
decoder.image_dist:         mse                                           (str)
decoder.vector_dist:        symlog_mse                                    (str)
decoder.inputs:             [deter, stoch]                                (strs)
decoder.resize:             stride                                        (str)
decoder.winit:              normal                                        (str)
decoder.fan:                avg                                           (str)
decoder.outscale:           1.0                                           (float)
decoder.minres:             4                                             (int)
decoder.cnn_sigmoid:        False                                         (bool)
reward_head.layers:         5                                             (int)
reward_head.units:          1024                                          (int)
reward_head.act:            silu                                          (str)
reward_head.norm:           layer                                         (str)
reward_head.dist:           symlog_disc                                   (str)
reward_head.outscale:       0.0                                           (float)
reward_head.outnorm:        False                                         (bool)
reward_head.inputs:         [deter, stoch]                                (strs)
reward_head.winit:          normal                                        (str)
reward_head.fan:            avg                                           (str)
reward_head.bins:           255                                           (int)
cont_head.layers:           5                                             (int)
cont_head.units:            1024                                          (int)
cont_head.act:              silu                                          (str)
cont_head.norm:             layer                                         (str)
cont_head.dist:             binary                                        (str)
cont_head.outscale:         1.0                                           (float)
cont_head.outnorm:          False                                         (bool)
cont_head.inputs:           [deter, stoch]                                (strs)
cont_head.winit:            normal                                        (str)
cont_head.fan:              avg                                           (str)
loss_scales.image:          1.0                                           (float)
loss_scales.vector:         1.0                                           (float)
loss_scales.reward:         1.0                                           (float)
loss_scales.cont:           1.0                                           (float)
loss_scales.dyn:            0.5                                           (float)
loss_scales.rep:            0.1                                           (float)
loss_scales.actor:          1.0                                           (float)
loss_scales.critic:         1.0                                           (float)
loss_scales.slowreg:        1.0                                           (float)
dyn_loss.impl:              kl                                            (str)
dyn_loss.free:              1.0                                           (float)
rep_loss.impl:              kl                                            (str)
rep_loss.free:              1.0                                           (float)
model_opt.opt:              adam                                          (str)
model_opt.lr:               0.0001                                        (float)
model_opt.eps:              1e-08                                         (float)
model_opt.clip:             1000.0                                        (float)
model_opt.wd:               0.0                                           (float)
model_opt.warmup:           0                                             (int)
model_opt.lateclip:         0.0                                           (float)
actor.layers:               5                                             (int)
actor.units:                1024                                          (int)
actor.act:                  silu                                          (str)
actor.norm:                 layer                                         (str)
actor.minstd:               0.1                                           (float)
actor.maxstd:               1.0                                           (float)
actor.outscale:             1.0                                           (float)
actor.outnorm:              False                                         (bool)
actor.unimix:               0.01                                          (float)
actor.inputs:               [deter, stoch]                                (strs)
actor.winit:                normal                                        (str)
actor.fan:                  avg                                           (str)
actor.symlog_inputs:        False                                         (bool)
critic.layers:              5                                             (int)
critic.units:               1024                                          (int)
critic.act:                 silu                                          (str)
critic.norm:                layer                                         (str)
critic.dist:                symlog_disc                                   (str)
critic.outscale:            0.0                                           (float)
critic.outnorm:             False                                         (bool)
critic.inputs:              [deter, stoch]                                (strs)
critic.winit:               normal                                        (str)
critic.fan:                 avg                                           (str)
critic.bins:                255                                           (int)
critic.symlog_inputs:       False                                         (bool)
actor_opt.opt:              adam                                          (str)
actor_opt.lr:               3e-05                                         (float)
actor_opt.eps:              1e-05                                         (float)
actor_opt.clip:             100.0                                         (float)
actor_opt.wd:               0.0                                           (float)
actor_opt.warmup:           0                                             (int)
actor_opt.lateclip:         0.0                                           (float)
critic_opt.opt:             adam                                          (str)
critic_opt.lr:              3e-05                                         (float)
critic_opt.eps:             1e-05                                         (float)
critic_opt.clip:            100.0                                         (float)
critic_opt.wd:              0.0                                           (float)
critic_opt.warmup:          0                                             (int)
critic_opt.lateclip:        0.0                                           (float)
actor_dist_disc:            onehot                                        (str)
actor_dist_cont:            normal                                        (str)
actor_grad_disc:            reinforce                                     (str)
actor_grad_cont:            backprop                                      (str)
critic_type:                vfunction                                     (str)
imag_horizon:               15                                            (int)
imag_unroll:                False                                         (bool)
horizon:                    333                                           (int)
return_lambda:              0.95                                          (float)
critic_slowreg:             logprob                                       (str)
slow_critic_update:         1                                             (int)
slow_critic_fraction:       0.02                                          (float)
retnorm.impl:               perc_ema                                      (str)
retnorm.decay:              0.99                                          (float)
retnorm.max:                1.0                                           (float)
retnorm.perclo:             5.0                                           (float)
retnorm.perchi:             95.0                                          (float)
actent:                     0.0003                                        (float)
expl_rewards.extr:          1.0                                           (float)
expl_rewards.disag:         0.1                                           (float)
expl_opt.opt:               adam                                          (str)
expl_opt.lr:                0.0001                                        (float)
expl_opt.eps:               1e-05                                         (float)
expl_opt.clip:              100.0                                         (float)
expl_opt.wd:                0.0                                           (float)
expl_opt.warmup:            0                                             (int)
disag_head.layers:          5                                             (int)
disag_head.units:           1024                                          (int)
disag_head.act:             silu                                          (str)
disag_head.norm:            layer                                         (str)
disag_head.dist:            mse                                           (str)
disag_head.outscale:        1.0                                           (float)
disag_head.inputs:          [deter, stoch, action]                        (strs)
disag_head.winit:           normal                                        (str)
disag_head.fan:             avg                                           (str)
disag_target:               [stoch]                                       (strs)
disag_models:               8                                             (int)wandb: Currently logged in as: ibisbill326 (ibisbill326-gmail-com). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/wandb/run-20240102_174325-gxddgecb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ziyu_crafter_cuda_0_seed_0
wandb: ⭐️ View project at https://wandb.ai/ibisbill326-gmail-com/crafter_reward
wandb: 🚀 View run at https://wandb.ai/ibisbill326-gmail-com/crafter_reward/runs/gxddgecb

!! Resuming wandb run !!
Encoder CNN shapes: {'image': (64, 64, 3)}
Encoder MLP shapes: {'transition_tokens': (384,)}
Decoder CNN shapes: {'image': (64, 64, 3)}
Decoder MLP shapes: {'transition_tokens': (384,)}
JAX devices (1): [cuda(id=0)]
Policy devices: cuda:0
Train devices:  cuda:0
Tracing train function.
no rnd data in data
Optimizer model_opt has 197,057,283 variables.
Optimizer actor_opt has 9,464,849 variables.
Optimizer critic_opt has 9,708,799 variables.
Logdir /home/ziyu/logdir/ziyu_crafter_cuda_0_seed_0
Observation space:
  image            Space(dtype=uint8, shape=(64, 64, 3), low=0, high=255)
  transition_tokens Space(dtype=uint32, shape=(384,), low=0, high=4294967295)
  goal_tokens      Space(dtype=uint32, shape=(5, 384), low=0, high=4294967295)
  goal_id          Space(dtype=uint32, shape=(5,), low=0, high=4294967295)
  reward           Space(dtype=float32, shape=(), low=-inf, high=inf)
  is_first         Space(dtype=bool, shape=(), low=False, high=True)
  is_last          Space(dtype=bool, shape=(), low=False, high=True)
  is_terminal      Space(dtype=bool, shape=(), low=False, high=True)
  log_reward       Space(dtype=float32, shape=(1,), low=-inf, high=inf)
  log_achievement_collect_coal Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_diamond Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_drink Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_iron Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_sapling Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_collect_wood Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_skeleton Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_defeat_zombie Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_cow Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_eat_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_iron_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_stone_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_pickaxe Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_make_wood_sword Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_furnace Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_plant Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_stone Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_place_table Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
  log_achievement_wake_up Space(dtype=int32, shape=(), low=-2147483648, high=2147483647)
Action space:
  action           Space(dtype=float32, shape=(17,), low=0, high=1)
  reset            Space(dtype=bool, shape=(), low=False, high=True)
Prefill train dataset.
Prefill eval dataset.
Found existing checkpoint.
Loading checkpoint: /home/ziyu/logdir/ziyu_crafter_cuda_0_seed_0/checkpoint.ckpt
Loaded checkpoint from 35867 seconds ago.
Start training loop.
Starting evaluation at step 30600
Tracing policy function.
Tracing policy function.
Episode has 186 steps and return 3.1.
Tracing policy function.
Tracing train function.
Tracing report function.
Tracing report function.
Tracing report function.
────────────────────────────────── Step 30601 ──────────────────────────────────
eval_episode/length 186 / eval_episode/score 3.1 / eval_episode/reward_rate 0.97
/ eval_stats/sum_log_reward 3.1 / eval_stats/max_log_achievement_collect_drink 
15 / eval_stats/max_log_achievement_collect_sapling 1 / 
eval_stats/max_log_achievement_place_plant 1 / 
eval_stats/max_log_achievement_wake_up 3 / train/action_mag 16 / 
train/action_max 16 / train/action_mean 4.48 / train/action_min 0 / 
train/action_std 3.46 / train/actor_opt_actor_opt_grad_overflow 0 / 
train/actor_opt_actor_opt_grad_scale 1e4 / train/actor_opt_grad_norm 0.06 / 
train/actor_opt_grad_steps 1.5e4 / train/actor_opt_loss 16.96 / train/adv_mag 
0.69 / train/adv_max 0.69 / train/adv_mean 5.6e-3 / train/adv_min -0.52 / 
train/adv_std 0.08 / train/cont_avg 1 / train/cont_loss_mean 1.7e-6 / 
train/cont_loss_std 2e-5 / train/cont_neg_acc 1 / train/cont_neg_loss 2.8e-4 / 
train/cont_pos_acc 1 / train/cont_pos_loss 2.9e-7 / train/cont_pred 1 / 
train/cont_rate 1 / train/dyn_loss_mean 4.56 / train/dyn_loss_std 8.02 / 
train/extr_critic_critic_opt_critic_opt_grad_overflow 0 / 
train/extr_critic_critic_opt_critic_opt_grad_scale 1e4 / 
train/extr_critic_critic_opt_grad_norm 1.18 / 
train/extr_critic_critic_opt_grad_steps 1.5e4 / 
train/extr_critic_critic_opt_loss 1.6e4 / train/extr_critic_mag 8.69 / 
train/extr_critic_max 8.69 / train/extr_critic_mean 2.01 / train/extr_critic_min
-0.34 / train/extr_critic_std 1.39 / train/extr_return_normed_mag 1.77 / 
train/extr_return_normed_max 1.77 / train/extr_return_normed_mean 0.39 / 
train/extr_return_normed_min -0.22 / train/extr_return_normed_std 0.31 / 
train/extr_return_rate 0.91 / train/extr_return_raw_mag 8.45 / 
train/extr_return_raw_max 8.45 / train/extr_return_raw_mean 2.04 / 
train/extr_return_raw_min -0.76 / train/extr_return_raw_std 1.42 / 
train/extr_reward_mag 1 / train/extr_reward_max 1 / train/extr_reward_mean 0.02 
/ train/extr_reward_min -0.71 / train/extr_reward_std 0.15 / 
train/image_loss_mean 5.9 / train/image_loss_std 10.13 / train/model_loss_mean 
8.67 / train/model_loss_std 13.77 / train/model_opt_grad_norm 78.36 / 
train/model_opt_grad_steps 1.5e4 / train/model_opt_loss 1.1e4 / 
train/model_opt_model_opt_grad_overflow 0 / train/model_opt_model_opt_grad_scale
1250 / train/policy_entropy_mag 2.37 / train/policy_entropy_max 2.37 / 
train/policy_entropy_mean 0.52 / train/policy_entropy_min 0.08 / 
train/policy_entropy_std 0.54 / train/policy_logprob_mag 7.44 / 
train/policy_logprob_max -9.5e-3 / train/policy_logprob_mean -0.52 / 
train/policy_logprob_min -7.44 / train/policy_logprob_std 1.08 / 
train/policy_randomness_mag 0.84 / train/policy_randomness_max 0.84 / 
train/policy_randomness_mean 0.18 / train/policy_randomness_min 0.03 / 
train/policy_randomness_std 0.19 / train/post_ent_mag 39.99 / train/post_ent_max
39.99 / train/post_ent_mean 23.47 / train/post_ent_min 11.62 / 
train/post_ent_std 5.1 / train/prior_ent_mag 66.82 / train/prior_ent_max 66.82 /
train/prior_ent_mean 27.58 / train/prior_ent_min 11.66 / train/prior_ent_std 
9.08 / train/rep_loss_mean 4.56 / train/rep_loss_std 8.02 / train/reward_avg 
0.01 / train/reward_loss_mean 0.04 / train/reward_loss_std 0.23 / 
train/reward_max_data 1 / train/reward_max_pred 1 / train/reward_neg_acc 1 / 
train/reward_neg_loss 0.02 / train/reward_pos_acc 1 / train/reward_pos_loss 0.76
/ train/reward_pred 0.01 / train/reward_rate 0.02 / 
train/transition_tokens_loss_mean 1.2e-4 / train/transition_tokens_loss_std 
4.2e-6 / train/params_agent/wm/model_opt 2e8 / 
train/params_agent/task_behavior/critic/critic_opt 9.7e6 / 
train/params_agent/task_behavior/ac/actor_opt 9.5e6 / report/cont_avg 1 / 
report/cont_loss_mean 2.6e-6 / report/cont_loss_std 3.3e-5 / report/cont_neg_acc
1 / report/cont_neg_loss 3.4e-4 / report/cont_pos_acc 1 / report/cont_pos_loss 
9.9e-7 / report/cont_pred 1 / report/cont_rate 1 / report/dyn_loss_mean 4.34 / 
report/dyn_loss_std 7.69 / report/image_loss_mean 5.32 / report/image_loss_std 
8.42 / report/model_loss_mean 7.96 / report/model_loss_std 11.91 / 
report/post_ent_mag 41.91 / report/post_ent_max 41.91 / report/post_ent_mean 
23.55 / report/post_ent_min 11.98 / report/post_ent_std 5.04 / 
report/prior_ent_mag 66.86 / report/prior_ent_max 66.86 / report/prior_ent_mean 
27.66 / report/prior_ent_min 11.14 / report/prior_ent_std 9.15 / 
report/rep_loss_mean 4.34 / report/rep_loss_std 7.69 / report/reward_avg 0.01 / 
report/reward_loss_mean 0.04 / report/reward_loss_std 0.23 / 
report/reward_max_data 1 / report/reward_max_pred 1 / report/reward_neg_acc 1 / 
report/reward_neg_loss 0.02 / report/reward_pos_acc 1 / report/reward_pos_loss 
0.69 / report/reward_pred 0.01 / report/reward_rate 0.02 / 
report/transition_tokens_loss_mean 9.6e-5 / report/transition_tokens_loss_std 
3.9e-6 / eval/cont_avg 1 / eval/cont_loss_mean 6.2e-3 / eval/cont_loss_std 0.2 /
eval/cont_neg_acc 0 / eval/cont_neg_loss 6.38 / eval/cont_pos_acc 1 / 
eval/cont_pos_loss 3e-7 / eval/cont_pred 1 / eval/cont_rate 1 / 
eval/dyn_loss_mean 20.03 / eval/dyn_loss_std 13.58 / eval/image_loss_mean 35.04 
/ eval/image_loss_std 52.85 / eval/model_loss_mean 47.11 / eval/model_loss_std 
57.6 / eval/post_ent_mag 39.27 / eval/post_ent_max 39.27 / eval/post_ent_mean 
25.36 / eval/post_ent_min 10.18 / eval/post_ent_std 5.59 / eval/prior_ent_mag 
66.86 / eval/prior_ent_max 66.86 / eval/prior_ent_mean 32.98 / 
eval/prior_ent_min 12.03 / eval/prior_ent_std 9.76 / eval/rep_loss_mean 20.03 / 
eval/rep_loss_std 13.58 / eval/reward_avg 8.7e-3 / eval/reward_loss_mean 0.05 / 
eval/reward_loss_std 0.6 / eval/reward_max_data 1 / eval/reward_max_pred 1 / 
eval/reward_neg_acc 1 / eval/reward_neg_loss 9.7e-3 / eval/reward_pos_acc 0.5 / 
eval/reward_pos_loss 4.29 / eval/reward_pred 3.4e-3 / eval/reward_rate 9.8e-3 / 
eval/transition_tokens_loss_mean 9.5e-5 / eval/transition_tokens_loss_std 3.5e-6
/ replay/size 7.8e4 / replay/inserts 0 / replay/samples 112 / 
replay/insert_wait_avg nan / replay/insert_wait_frac nan / 
replay/sample_wait_avg 1.8e-6 / replay/sample_wait_frac 1 / eval_replay/size 
2954 / eval_replay/inserts 124 / eval_replay/samples 112 / 
eval_replay/insert_wait_avg 4e-6 / eval_replay/insert_wait_frac 1 / 
eval_replay/sample_wait_avg 2.3e-6 / eval_replay/sample_wait_frac 1 / 
timer/duration 163.33 / timer/replay._sample_count 112 / 
timer/replay._sample_total 20.62 / timer/replay._sample_frac 0.13 / 
timer/replay._sample_avg 0.18 / timer/replay._sample_min 0.06 / 
timer/replay._sample_max 1.27 / timer/agent.policy_count 188 / 
timer/agent.policy_total 8.86 / timer/agent.policy_frac 0.05 / 
timer/agent.policy_avg 0.05 / timer/agent.policy_min 8.4e-3 / 
timer/agent.policy_max 5.06 / timer/env.step_count 1 / timer/env.step_total 1.41
/ timer/env.step_frac 8.6e-3 / timer/env.step_avg 1.41 / timer/env.step_min 1.41
/ timer/env.step_max 1.41 / timer/dataset_train_count 1 / 
timer/dataset_train_total 6.5e-5 / timer/dataset_train_frac 4e-7 / 
timer/dataset_train_avg 6.5e-5 / timer/dataset_train_min 6.5e-5 / 
timer/dataset_train_max 6.5e-5 / timer/agent.train_count 1 / 
timer/agent.train_total 100.03 / timer/agent.train_frac 0.61 / 
timer/agent.train_avg 100.03 / timer/agent.train_min 100.03 / 
timer/agent.train_max 100.03 / timer/agent.report_count 2 / 
timer/agent.report_total 22.21 / timer/agent.report_frac 0.14 / 
timer/agent.report_avg 11.1 / timer/agent.report_min 6.2 / 
timer/agent.report_max 16.01 / timer/dataset_eval_count 1 / 
timer/dataset_eval_total 7.3e-5 / timer/dataset_eval_frac 4.5e-7 / 
timer/dataset_eval_avg 7.3e-5 / timer/dataset_eval_min 7.3e-5 / 
timer/dataset_eval_max 7.3e-5
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/ziyu/logdir/ziyu_crafter_cuda_0_seed_0

Creating new TensorBoard event file writer.
Episode has 163 steps and return 0.1.
Episode has 424 steps and return 6.1.
Episode has 176 steps and return 4.1.
────────────────────────────────── Step 20445 ──────────────────────────────────
episode/length 176 / episode/score 4.1 / episode/reward_rate 0.99 / 
train/action_mag 16 / train/action_max 16 / train/action_mean 6.85 / 
train/action_min 0 / train/action_std 4.95 / 
train/actor_opt_actor_opt_grad_overflow 0 / train/actor_opt_actor_opt_grad_scale
1e4 / train/actor_opt_grad_norm 0.04 / train/actor_opt_grad_steps 9915 / 
train/actor_opt_loss -34.15 / train/adv_mag 0.77 / train/adv_max 0.75 / 
train/adv_mean -1.4e-3 / train/adv_min -0.56 / train/adv_std 0.06 / 
train/cont_avg 0.99 / train/cont_loss_mean 1.6e-4 / train/cont_loss_std 4.2e-3 /
train/cont_neg_acc 0.99 / train/cont_neg_loss 0.02 / train/cont_pos_acc 1 / 
train/cont_pos_loss 8.4e-5 / train/cont_pred 0.99 / train/cont_rate 0.99 / 
train/dyn_loss_mean 4.59 / train/dyn_loss_std 7.42 / 
train/extr_critic_critic_opt_critic_opt_grad_overflow 0 / 
train/extr_critic_critic_opt_critic_opt_grad_scale 1e4 / 
train/extr_critic_critic_opt_grad_norm 1.41 / 
train/extr_critic_critic_opt_grad_steps 9915 / train/extr_critic_critic_opt_loss
1.6e4 / train/extr_critic_mag 8.98 / train/extr_critic_max 8.98 / 
train/extr_critic_mean 1.58 / train/extr_critic_min -0.53 / 
train/extr_critic_std 1.75 / train/extr_return_normed_mag 1.73 / 
train/extr_return_normed_max 1.73 / train/extr_return_normed_mean 0.32 / 
train/extr_return_normed_min -0.13 / train/extr_return_normed_std 0.32 / 
train/extr_return_rate 0.63 / train/extr_return_raw_mag 9.45 / 
train/extr_return_raw_max 9.45 / train/extr_return_raw_mean 1.58 / 
train/extr_return_raw_min -0.95 / train/extr_return_raw_std 1.79 / 
train/extr_reward_mag 1.02 / train/extr_reward_max 1.02 / train/extr_reward_mean
0.02 / train/extr_reward_min -0.68 / train/extr_reward_std 0.14 / 
train/image_loss_mean 6.6 / train/image_loss_std 11.17 / train/model_loss_mean 
9.39 / train/model_loss_std 14.29 / train/model_opt_grad_norm 91.54 / 
train/model_opt_grad_steps 9901.07 / train/model_opt_loss 3319.32 / 
train/model_opt_model_opt_grad_overflow 0 / train/model_opt_model_opt_grad_scale
334.82 / train/policy_entropy_mag 2.43 / train/policy_entropy_max 2.43 / 
train/policy_entropy_mean 0.71 / train/policy_entropy_min 0.08 / 
train/policy_entropy_std 0.58 / train/policy_logprob_mag 7.44 / 
train/policy_logprob_max -9.5e-3 / train/policy_logprob_mean -0.72 / 
train/policy_logprob_min -7.44 / train/policy_logprob_std 1.18 / 
train/policy_randomness_mag 0.86 / train/policy_randomness_max 0.86 / 
train/policy_randomness_mean 0.25 / train/policy_randomness_min 0.03 / 
train/policy_randomness_std 0.2 / train/post_ent_mag 37.24 / train/post_ent_max 
37.24 / train/post_ent_mean 22.9 / train/post_ent_min 9.74 / train/post_ent_std 
4.78 / train/prior_ent_mag 64.82 / train/prior_ent_max 64.82 / 
train/prior_ent_mean 27.51 / train/prior_ent_min 11.62 / train/prior_ent_std 
8.65 / train/rep_loss_mean 4.59 / train/rep_loss_std 7.42 / train/reward_avg 
0.01 / train/reward_loss_mean 0.04 / train/reward_loss_std 0.2 / 
train/reward_max_data 1 / train/reward_max_pred 1 / train/reward_neg_acc 1 / 
train/reward_neg_loss 0.02 / train/reward_pos_acc 0.98 / train/reward_pos_loss 
0.78 / train/reward_pred 0.01 / train/reward_rate 0.02 / 
train/transition_tokens_loss_mean 6.5e-4 / train/transition_tokens_loss_std 
3.1e-5 / train_stats/sum_log_reward 3.43 / 
train_stats/max_log_achievement_collect_sapling 0.67 / 
train_stats/max_log_achievement_place_plant 0.67 / 
train_stats/max_log_achievement_wake_up 3.33 / train_stats/mean_log_entropy 0.74
/ train_stats/max_log_achievement_collect_drink 11.5 / 
train_stats/max_log_achievement_collect_wood 3.5 / 
train_stats/max_log_achievement_make_wood_sword 1 / 
train_stats/max_log_achievement_place_table 1 / report/cont_avg 0.99 / 
report/cont_loss_mean 7e-5 / report/cont_loss_std 9.6e-4 / report/cont_neg_acc 1
/ report/cont_neg_loss 1.8e-4 / report/cont_pos_acc 1 / report/cont_pos_loss 
6.9e-5 / report/cont_pred 0.99 / report/cont_rate 0.99 / report/dyn_loss_mean 
4.37 / report/dyn_loss_std 7.47 / report/image_loss_mean 5.4 / 
report/image_loss_std 7.99 / report/model_loss_mean 8.1 / report/model_loss_std 
11.15 / report/post_ent_mag 39.31 / report/post_ent_max 39.31 / 
report/post_ent_mean 23.08 / report/post_ent_min 10.87 / report/post_ent_std 
5.17 / report/prior_ent_mag 63.21 / report/prior_ent_max 63.21 / 
report/prior_ent_mean 27.93 / report/prior_ent_min 13.25 / report/prior_ent_std 
9.26 / report/rep_loss_mean 4.37 / report/rep_loss_std 7.47 / report/reward_avg 
0.03 / report/reward_loss_mean 0.07 / report/reward_loss_std 0.37 / 
report/reward_max_data 1 / report/reward_max_pred 1 / report/reward_neg_acc 1 / 
report/reward_neg_loss 0.05 / report/reward_pos_acc 1 / report/reward_pos_loss 
0.71 / report/reward_pred 0.03 / report/reward_rate 0.03 / 
report/transition_tokens_loss_mean 8.6e-4 / report/transition_tokens_loss_std 
3.3e-5 / eval/cont_avg 0.99 / eval/cont_loss_mean 2.2e-4 / eval/cont_loss_std 
4.4e-3 / eval/cont_neg_acc 1 / eval/cont_neg_loss 0.01 / eval/cont_pos_acc 1 / 
eval/cont_pos_loss 1.4e-4 / eval/cont_pred 0.99 / eval/cont_rate 0.99 / 
eval/dyn_loss_mean 22.22 / eval/dyn_loss_std 11.44 / eval/image_loss_mean 46.8 /
eval/image_loss_std 46.02 / eval/model_loss_mean 60.23 / eval/model_loss_std 
48.97 / eval/post_ent_mag 37.87 / eval/post_ent_max 37.87 / eval/post_ent_mean 
25.17 / eval/post_ent_min 10.28 / eval/post_ent_std 4.88 / eval/prior_ent_mag 
63.21 / eval/prior_ent_max 63.21 / eval/prior_ent_mean 32.03 / 
eval/prior_ent_min 11.23 / eval/prior_ent_std 9.12 / eval/rep_loss_mean 22.22 / 
eval/rep_loss_std 11.44 / eval/reward_avg 7.9e-3 / eval/reward_loss_mean 0.1 / 
eval/reward_loss_std 0.67 / eval/reward_max_data 1 / eval/reward_max_pred 1.01 /
eval/reward_neg_acc 1 / eval/reward_neg_loss 0.08 / eval/reward_pos_acc 0.85 / 
eval/reward_pos_loss 1.69 / eval/reward_pred 6e-3 / eval/reward_rate 0.01 / 
eval/transition_tokens_loss_mean 8.5e-4 / eval/transition_tokens_loss_std 2.9e-5
/ replay/size 5.6e4 / replay/inserts 782 / replay/samples 6752 / 
replay/insert_wait_avg 4e-6 / replay/insert_wait_frac 1 / replay/sample_wait_avg
1.2e-6 / replay/sample_wait_frac 1 / eval_replay/size 2692 / eval_replay/inserts
0 / eval_replay/samples 16 / eval_replay/insert_wait_avg nan / 
eval_replay/insert_wait_frac nan / eval_replay/sample_wait_avg 1.8e-6 / 
eval_replay/sample_wait_frac 1 / timer/duration 278.55 / 
timer/replay._sample_count 6752 / timer/replay._sample_total 367.16 / 
timer/replay._sample_frac 1.32 / timer/replay._sample_avg 0.05 / 
timer/replay._sample_min 6.8e-4 / timer/replay._sample_max 0.1 / 
timer/agent.policy_count 844 / timer/agent.policy_total 8.7 / 
timer/agent.policy_frac 0.03 / timer/agent.policy_avg 0.01 / 
timer/agent.policy_min 8.4e-3 / timer/agent.policy_max 0.02 / 
timer/env.step_count 844 / timer/env.step_total 87.04 / timer/env.step_frac 0.31
/ timer/env.step_avg 0.1 / timer/env.step_min 2.6e-3 / timer/env.step_max 1.37 /
timer/dataset_train_count 422 / timer/dataset_train_total 0.06 / 
timer/dataset_train_frac 2e-4 / timer/dataset_train_avg 1.3e-4 / 
timer/dataset_train_min 1.1e-4 / timer/dataset_train_max 5.5e-4 / 
timer/agent.train_count 422 / timer/agent.train_total 170.24 / 
timer/agent.train_frac 0.61 / timer/agent.train_avg 0.4 / timer/agent.train_min 
0.39 / timer/agent.train_max 0.43 / timer/agent.report_count 2 / 
timer/agent.report_total 0.46 / timer/agent.report_frac 1.6e-3 / 
timer/agent.report_avg 0.23 / timer/agent.report_min 0.23 / 
timer/agent.report_max 0.23 / timer/dataset_eval_count 1 / 
timer/dataset_eval_total 5.6e-5 / timer/dataset_eval_frac 2e-7 / 
timer/dataset_eval_avg 5.6e-5 / timer/dataset_eval_min 5.6e-5 / 
timer/dataset_eval_max 5.6e-5 / fps 3.03

Creating new TensorBoard event file writer.
Episode has 116 steps and return 3.1.
Episode has 187 steps and return 2.1.
Episode has 220 steps and return 4.1.
Traceback (most recent call last):
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py", line 229, in <module>
    main()
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py", line 69, in main
    embodied.run.train_eval(
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/train_eval.py", line 146, in train_eval
    driver_train(policy_train, steps=100)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 42, in __call__
    step, episode = self._step(policy, step, episode)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 65, in _step
    [fn(trn, i, **self._kwargs) for fn in self._on_steps]
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 65, in <listcomp>
    [fn(trn, i, **self._kwargs) for fn in self._on_steps]
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/train_eval.py", line 108, in train_step
    outs, state[0], mets = agent.train(batch[0], state[0])
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/jaxagent.py", line 75, in train
    (outs, state, mets), self.varibs = self._train(
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/ninjax.py", line 208, in wrapper
    out, updated = apply(statics, selected, rng, *args, **kw)
  File "<string>", line 1, in <lambda>
KeyboardInterrupt
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py:229 in   │
│ <module>                                                                     │
│                                                                              │
│   226                                                                        │
│   227                                                                        │
│   228 if __name__ == '__main__':                                             │
│ ❱ 229   main()                                                               │
│   230                                                                        │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py:69 in    │
│ main                                                                         │
│                                                                              │
│    66 │     eval_env = make_envs(config)  # mode='eval'                      │
│    67 │     cleanup += [env, eval_env]                                       │
│    68 │     agent = agt.Agent(env.obs_space, env.act_space, step, config)    │
│ ❱  69 │     embodied.run.train_eval(                                         │
│    70 │   │     agent, env, eval_env, replay, eval_replay, logger, args)     │
│    71 │                                                                      │
│    72 │   elif args.script == 'train_holdout':                               │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/trai │
│ n_eval.py:146 in train_eval                                                  │
│                                                                              │
│   143 │     print('Starting evaluation at step', int(step))                  │
│   144 │     driver_eval.reset()                                              │
│   145 │     driver_eval(policy_eval, episodes=max(len(eval_env), args.eval_e │
│ ❱ 146 │   driver_train(policy_train, steps=100)                              │
│   147 │   if should_save(step):                                              │
│   148 │     checkpoint.save()                                                │
│   149   logger.write()                                                       │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:42 in __call__                                                        │
│                                                                              │
│   39   def __call__(self, policy, steps=0, episodes=0):                      │
│   40 │   step, episode = 0, 0                                                │
│   41 │   while step < steps or episode < episodes:                           │
│ ❱ 42 │     step, episode = self._step(policy, step, episode)                 │
│   43                                                                         │
│   44   def _step(self, policy, step, episode):                               │
│   45 │   assert all(len(x) == len(self._env) for x in self._acts.values())   │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:65 in _step                                                           │
│                                                                              │
│   62 │   for i in range(len(self._env)):                                     │
│   63 │     trn = {k: v[i] for k, v in trns.items()}                          │
│   64 │     [self._eps[i][k].append(v) for k, v in trn.items()]               │
│ ❱ 65 │     [fn(trn, i, **self._kwargs) for fn in self._on_steps]             │
│   66 │     step += 1                                                         │
│   67 │   if obs['is_last'].any():                                            │
│   68 │     for i, done in enumerate(obs['is_last']):                         │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:65 in <listcomp>                                                      │
│                                                                              │
│   62 │   for i in range(len(self._env)):                                     │
│   63 │     trn = {k: v[i] for k, v in trns.items()}                          │
│   64 │     [self._eps[i][k].append(v) for k, v in trn.items()]               │
│ ❱ 65 │     [fn(trn, i, **self._kwargs) for fn in self._on_steps]             │
│   66 │     step += 1                                                         │
│   67 │   if obs['is_last'].any():                                            │
│   68 │     for i, done in enumerate(obs['is_last']):                         │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/trai │
│ n_eval.py:108 in train_step                                                  │
│                                                                              │
│   105 │     #----give intrinsic rewards to batch sampled>>>>                 │
│   106 │     batch[0]['rnd'] = get_intrinsic_and_update_rnd(batch)            │
│   107 │     #-------------------------------------------<<<<                 │
│ ❱ 108 │     outs, state[0], mets = agent.train(batch[0], state[0])           │
│   109 │     metrics.add(mets, prefix='train')                                │
│   110 │     if 'priority' in outs:                                           │
│   111 │   │   train_replay.prioritize(outs['key'], outs['priority'])         │
│                                                                              │
│ /home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/contextlib.py:79 in inner   │
│                                                                              │
│    76 │   │   @wraps(func)                                                   │
│    77 │   │   def inner(*args, **kwds):                                      │
│    78 │   │   │   with self._recreate_cm():                                  │
│ ❱  79 │   │   │   │   return func(*args, **kwds)                             │
│    80 │   │   return inner                                                   │
│    81                                                                        │
│    82                                                                        │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/jaxagent.py:75 in │
│ train                                                                        │
│                                                                              │
│    72 │   #----                                                              │
│    73 │   data = self._convert_inps(data, self.policy_devices)               │
│    74 │   #----                                                              │
│ ❱  75 │   (outs, state, mets), self.varibs = self._train(                    │
│    76 │   │   self.varibs, rng, data, state)                                 │
│    77 │   outs = self._convert_outs(outs, self.train_devices)                │
│    78 │   self._updates.increment()                                          │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/ninjax.py:208 in  │
│ wrapper                                                                      │
│                                                                              │
│   205 │     return state                                                     │
│   206 │   else:                                                              │
│   207 │     selected = {k: v for k, v in state.items() if k in wrapper.keys} │
│ ❱ 208 │     out, updated = apply(statics, selected, rng, *args, **kw)        │
│   209 │     return out, {**state, **updated}                                 │
│   210   return wrapper                                                       │
│   211                                                                        │
│ in <lambda>:1                                                                │
╰──────────────────────────────────────────────────────────────────────────────╯
KeyboardInterrupt
Exception ignored in atexit callback: <function _Manager._atexit_setup.<locals>.<lambda> at 0x7ff751fcba30>
Traceback (most recent call last):
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 155, in <lambda>
    self._atexit_lambda = lambda: self._atexit_teardown()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 164, in _atexit_teardown
    self._teardown(exit_code)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 175, in _teardown
    result = self._service.join()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/service/service.py", line 271, in join
    ret = self._internal_proc.wait()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt: 

Episode has 197 steps and return 3.1.
Saved chunk: 20240102T174055F762558-2Y947mGP6FRpxNWeb27E9U-5L3diK9FrI07EGo4W24oeZ-1024.npz
Episode has 210 steps and return 4.1.
Episode has 176 steps and return 4.1.
Episode has 171 steps and return 3.1.
Traceback (most recent call last):
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py", line 229, in <module>
    main()
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py", line 69, in main
    embodied.run.train_eval(
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/train_eval.py", line 146, in train_eval
    driver_train(policy_train, steps=100)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 42, in __call__
    step, episode = self._step(policy, step, episode)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 65, in _step
    [fn(trn, i, **self._kwargs) for fn in self._on_steps]
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/driver.py", line 65, in <listcomp>
    [fn(trn, i, **self._kwargs) for fn in self._on_steps]
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/train_eval.py", line 108, in train_step
    outs, state[0], mets = agent.train(batch[0], state[0])
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/jaxagent.py", line 75, in train
    (outs, state, mets), self.varibs = self._train(
  File "/home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/ninjax.py", line 208, in wrapper
    out, updated = apply(statics, selected, rng, *args, **kw)
  File "<string>", line 1, in <lambda>
KeyboardInterrupt
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py:229 in   │
│ <module>                                                                     │
│                                                                              │
│   226                                                                        │
│   227                                                                        │
│   228 if __name__ == '__main__':                                             │
│ ❱ 229   main()                                                               │
│   230                                                                        │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/train.py:69 in    │
│ main                                                                         │
│                                                                              │
│    66 │     eval_env = make_envs(config)  # mode='eval'                      │
│    67 │     cleanup += [env, eval_env]                                       │
│    68 │     agent = agt.Agent(env.obs_space, env.act_space, step, config)    │
│ ❱  69 │     embodied.run.train_eval(                                         │
│    70 │   │     agent, env, eval_env, replay, eval_replay, logger, args)     │
│    71 │                                                                      │
│    72 │   elif args.script == 'train_holdout':                               │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/trai │
│ n_eval.py:146 in train_eval                                                  │
│                                                                              │
│   143 │     print('Starting evaluation at step', int(step))                  │
│   144 │     driver_eval.reset()                                              │
│   145 │     driver_eval(policy_eval, episodes=max(len(eval_env), args.eval_e │
│ ❱ 146 │   driver_train(policy_train, steps=100)                              │
│   147 │   if should_save(step):                                              │
│   148 │     checkpoint.save()                                                │
│   149   logger.write()                                                       │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:42 in __call__                                                        │
│                                                                              │
│   39   def __call__(self, policy, steps=0, episodes=0):                      │
│   40 │   step, episode = 0, 0                                                │
│   41 │   while step < steps or episode < episodes:                           │
│ ❱ 42 │     step, episode = self._step(policy, step, episode)                 │
│   43                                                                         │
│   44   def _step(self, policy, step, episode):                               │
│   45 │   assert all(len(x) == len(self._env) for x in self._acts.values())   │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:65 in _step                                                           │
│                                                                              │
│   62 │   for i in range(len(self._env)):                                     │
│   63 │     trn = {k: v[i] for k, v in trns.items()}                          │
│   64 │     [self._eps[i][k].append(v) for k, v in trn.items()]               │
│ ❱ 65 │     [fn(trn, i, **self._kwargs) for fn in self._on_steps]             │
│   66 │     step += 1                                                         │
│   67 │   if obs['is_last'].any():                                            │
│   68 │     for i, done in enumerate(obs['is_last']):                         │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/core/dri │
│ ver.py:65 in <listcomp>                                                      │
│                                                                              │
│   62 │   for i in range(len(self._env)):                                     │
│   63 │     trn = {k: v[i] for k, v in trns.items()}                          │
│   64 │     [self._eps[i][k].append(v) for k, v in trn.items()]               │
│ ❱ 65 │     [fn(trn, i, **self._kwargs) for fn in self._on_steps]             │
│   66 │     step += 1                                                         │
│   67 │   if obs['is_last'].any():                                            │
│   68 │     for i, done in enumerate(obs['is_last']):                         │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/embodied/run/trai │
│ n_eval.py:108 in train_step                                                  │
│                                                                              │
│   105 │     #----give intrinsic rewards to batch sampled>>>>                 │
│   106 │     batch[0]['rnd'] = get_intrinsic_and_update_rnd(batch)            │
│   107 │     #-------------------------------------------<<<<                 │
│ ❱ 108 │     outs, state[0], mets = agent.train(batch[0], state[0])           │
│   109 │     metrics.add(mets, prefix='train')                                │
│   110 │     if 'priority' in outs:                                           │
│   111 │   │   train_replay.prioritize(outs['key'], outs['priority'])         │
│                                                                              │
│ /home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/contextlib.py:79 in inner   │
│                                                                              │
│    76 │   │   @wraps(func)                                                   │
│    77 │   │   def inner(*args, **kwds):                                      │
│    78 │   │   │   with self._recreate_cm():                                  │
│ ❱  79 │   │   │   │   return func(*args, **kwds)                             │
│    80 │   │   return inner                                                   │
│    81                                                                        │
│    82                                                                        │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/jaxagent.py:75 in │
│ train                                                                        │
│                                                                              │
│    72 │   #----                                                              │
│    73 │   data = self._convert_inps(data, self.policy_devices)               │
│    74 │   #----                                                              │
│ ❱  75 │   (outs, state, mets), self.varibs = self._train(                    │
│    76 │   │   self.varibs, rng, data, state)                                 │
│    77 │   outs = self._convert_outs(outs, self.train_devices)                │
│    78 │   self._updates.increment()                                          │
│                                                                              │
│ /home/ziyu/code/side_codes/Dynamic_model/DLLM-ziyu/LID-rnd/ninjax.py:208 in  │
│ wrapper                                                                      │
│                                                                              │
│   205 │     return state                                                     │
│   206 │   else:                                                              │
│   207 │     selected = {k: v for k, v in state.items() if k in wrapper.keys} │
│ ❱ 208 │     out, updated = apply(statics, selected, rng, *args, **kw)        │
│   209 │     return out, {**state, **updated}                                 │
│   210   return wrapper                                                       │
│   211                                                                        │
│ in <lambda>:1                                                                │
╰──────────────────────────────────────────────────────────────────────────────╯
KeyboardInterrupt
wandb: WARNING No requirements.txt found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
Exception ignored in atexit callback: <function _Manager._atexit_setup.<locals>.<lambda> at 0x7f607ab87a30>
Traceback (most recent call last):
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 155, in <lambda>
    self._atexit_lambda = lambda: self._atexit_teardown()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 164, in _atexit_teardown
    self._teardown(exit_code)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/wandb_manager.py", line 175, in _teardown
    result = self._service.join()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/site-packages/wandb/sdk/service/service.py", line 271, in join
    ret = self._internal_proc.wait()
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/ziyu/anaconda3/envs/jaxpy39/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt: 
wandb: 
wandb: Run history:
wandb:                                        episode/length ▁█▁
wandb:                                   episode/reward_rate ▁▂█
wandb:                                         episode/score ▁█▆
wandb:                                         eval/cont_avg ▁▁
wandb:                                   eval/cont_loss_mean █▁
wandb:                                    eval/cont_loss_std █▁
wandb:                                     eval/cont_neg_acc ▁▁
wandb:                                    eval/cont_neg_loss █▁
wandb:                                     eval/cont_pos_acc ▁█
wandb:                                    eval/cont_pos_loss █▁
wandb:                                        eval/cont_pred ▁█
wandb:                                        eval/cont_rate ▁▁
wandb:                                    eval/dyn_loss_mean █▁
wandb:                                     eval/dyn_loss_std █▁
wandb:                                  eval/image_loss_mean █▁
wandb:                                   eval/image_loss_std █▁
wandb:                                  eval/model_loss_mean █▁
wandb:                                   eval/model_loss_std █▁
wandb:                                     eval/post_ent_mag █▁
wandb:                                     eval/post_ent_max █▁
wandb:                                    eval/post_ent_mean █▁
wandb:                                     eval/post_ent_min █▁
wandb:                                     eval/post_ent_std █▁
wandb:                                    eval/prior_ent_mag █▁
wandb:                                    eval/prior_ent_max █▁
wandb:                                   eval/prior_ent_mean ▁█
wandb:                                    eval/prior_ent_min ▁█
wandb:                                    eval/prior_ent_std ▁█
wandb:                                    eval/rep_loss_mean █▁
wandb:                                     eval/rep_loss_std █▁
wandb:                                       eval/reward_avg ▁█
wandb:                                 eval/reward_loss_mean █▁
wandb:                                  eval/reward_loss_std █▁
wandb:                                  eval/reward_max_data ▁▁
wandb:                                  eval/reward_max_pred ▁█
wandb:                                   eval/reward_neg_acc ▁█
wandb:                                  eval/reward_neg_loss █▁
wandb:                                   eval/reward_pos_acc ▁█
wandb:                                  eval/reward_pos_loss █▁
wandb:                                      eval/reward_pred ▁█
wandb:                                      eval/reward_rate ▁█
wandb:                      eval/transition_tokens_loss_mean █▁
wandb:                       eval/transition_tokens_loss_std █▁
wandb:                                   eval_episode/length ▁
wandb:                              eval_episode/reward_rate ▁
wandb:                                    eval_episode/score ▁
wandb:                                   eval_replay/inserts ▁▁
wandb:                           eval_replay/sample_wait_avg █▁
wandb:                          eval_replay/sample_wait_frac ▁▁
wandb:                                   eval_replay/samples █▁
wandb:                                      eval_replay/size ▁▁
wandb:        eval_stats/max_log_achievement_collect_sapling ▁
wandb:            eval_stats/max_log_achievement_place_plant ▁
wandb:                             eval_stats/sum_log_reward ▁
wandb:                                                   fps ▁
wandb:                                           global_step ▁▁▂▆▇█
wandb:                                replay/insert_wait_avg ▁
wandb:                               replay/insert_wait_frac ▁
wandb:                                        replay/inserts ▁█
wandb:                                replay/sample_wait_avg █▁
wandb:                               replay/sample_wait_frac ▁▁
wandb:                                        replay/samples ▁█
wandb:                                           replay/size ▁█
wandb:                                       report/cont_avg ▁▁
wandb:                                 report/cont_loss_mean █▁
wandb:                                  report/cont_loss_std █▁
wandb:                                   report/cont_neg_acc ▁▁
wandb:                                  report/cont_neg_loss █▁
wandb:                                   report/cont_pos_acc ▁▁
wandb:                                  report/cont_pos_loss █▁
wandb:                                      report/cont_pred █▁
wandb:                                      report/cont_rate ▁▁
wandb:                                  report/dyn_loss_mean █▁
wandb:                                   report/dyn_loss_std █▁
wandb:                                report/image_loss_mean █▁
wandb:                                 report/image_loss_std █▁
wandb:                                report/model_loss_mean █▁
wandb:                                 report/model_loss_std █▁
wandb:                                   report/post_ent_mag █▁
wandb:                                   report/post_ent_max █▁
wandb:                                  report/post_ent_mean ▁█
wandb:                                   report/post_ent_min ▁█
wandb:                                   report/post_ent_std ▁█
wandb:                                  report/prior_ent_mag █▁
wandb:                                  report/prior_ent_max █▁
wandb:                                 report/prior_ent_mean ▁█
wandb:                                  report/prior_ent_min ▁█
wandb:                                  report/prior_ent_std ▁█
wandb:                                  report/rep_loss_mean █▁
wandb:                                   report/rep_loss_std █▁
wandb:                                     report/reward_avg ▁█
wandb:                               report/reward_loss_mean ▁█
wandb:                                report/reward_loss_std ▁█
wandb:                                report/reward_max_data ▁▁
wandb:                                report/reward_max_pred █▁
wandb:                                 report/reward_neg_acc ▁█
wandb:                                report/reward_neg_loss ▁█
wandb:                                 report/reward_pos_acc ▁█
wandb:                                report/reward_pos_loss █▁
wandb:                                    report/reward_pred ▁█
wandb:                                    report/reward_rate ▁█
wandb:                    report/transition_tokens_loss_mean █▁
wandb:                     report/transition_tokens_loss_std █▁
wandb:                                timer/agent.policy_avg █▁
wandb:                              timer/agent.policy_count ▁█
wandb:                               timer/agent.policy_frac █▁
wandb:                                timer/agent.policy_max █▁
wandb:                                timer/agent.policy_min █▁
wandb:                              timer/agent.policy_total ▁█
wandb:                                timer/agent.report_avg █▁
wandb:                              timer/agent.report_count ▁▁
wandb:                               timer/agent.report_frac █▁
wandb:                                timer/agent.report_max █▁
wandb:                                timer/agent.report_min █▁
wandb:                              timer/agent.report_total █▁
wandb:                                 timer/agent.train_avg █▁
wandb:                               timer/agent.train_count ▁█
wandb:                                timer/agent.train_frac █▁
wandb:                                 timer/agent.train_max █▁
wandb:                                 timer/agent.train_min █▁
wandb:                               timer/agent.train_total ▁█
wandb:                                timer/dataset_eval_avg █▁
wandb:                              timer/dataset_eval_count ▁▁
wandb:                               timer/dataset_eval_frac █▁
wandb:                                timer/dataset_eval_max █▁
wandb:                                timer/dataset_eval_min █▁
wandb:                              timer/dataset_eval_total █▁
wandb:                               timer/dataset_train_avg ▁█
wandb:                             timer/dataset_train_count ▁█
wandb:                              timer/dataset_train_frac ▁█
wandb:                               timer/dataset_train_max ▁█
wandb:                               timer/dataset_train_min ▁█
wandb:                             timer/dataset_train_total ▁█
wandb:                                        timer/duration ▁█
wandb:                                    timer/env.step_avg █▁
wandb:                                  timer/env.step_count ▁█
wandb:                                   timer/env.step_frac ▁█
wandb:                                    timer/env.step_max ▁█
wandb:                                    timer/env.step_min █▁
wandb:                                  timer/env.step_total ▁█
wandb:                              timer/replay._sample_avg █▁
wandb:                            timer/replay._sample_count ▁█
wandb:                             timer/replay._sample_frac ▁█
wandb:                              timer/replay._sample_max █▁
wandb:                              timer/replay._sample_min █▁
wandb:                            timer/replay._sample_total ▁█
wandb:                                      train/action_mag ▁▁
wandb:                                      train/action_max ▁▁
wandb:                                     train/action_mean ▁█
wandb:                                      train/action_min ▁▁
wandb:                                      train/action_std ▁█
wandb:               train/actor_opt_actor_opt_grad_overflow ▁▁
wandb:                  train/actor_opt_actor_opt_grad_scale ▁▁
wandb:                             train/actor_opt_grad_norm ▁█
wandb:                            train/actor_opt_grad_steps ▁█
wandb:                                  train/actor_opt_loss █▁
wandb:                                         train/adv_mag ▁█
wandb:                                         train/adv_max ▁█
wandb:                                        train/adv_mean █▁
wandb:                                         train/adv_min █▁
wandb:                                         train/adv_std ▁█
wandb:                                        train/cont_avg ▁█
wandb:                                  train/cont_loss_mean █▁
wandb:                                   train/cont_loss_std █▁
wandb:                                    train/cont_neg_acc ▁█
wandb:                                   train/cont_neg_loss █▁
wandb:                                    train/cont_pos_acc █▁
wandb:                                   train/cont_pos_loss █▁
wandb:                                       train/cont_pred ▁█
wandb:                                       train/cont_rate ▁█
wandb:                                   train/dyn_loss_mean █▁
wandb:                                    train/dyn_loss_std █▁
wandb: train/extr_critic_critic_opt_critic_opt_grad_overflow ▁▁
wandb:    train/extr_critic_critic_opt_critic_opt_grad_scale ▁▁
wandb:                train/extr_critic_critic_opt_grad_norm █▁
wandb:               train/extr_critic_critic_opt_grad_steps ▁█
wandb:                     train/extr_critic_critic_opt_loss █▁
wandb:                                 train/extr_critic_mag ▁█
wandb:                                 train/extr_critic_max ▁█
wandb:                                train/extr_critic_mean █▁
wandb:                                 train/extr_critic_min ▁█
wandb:                                 train/extr_critic_std █▁
wandb:                          train/extr_return_normed_mag ▁█
wandb:                          train/extr_return_normed_max ▁█
wandb:                         train/extr_return_normed_mean █▁
wandb:                          train/extr_return_normed_min █▁
wandb:                          train/extr_return_normed_std █▁
wandb:                                train/extr_return_rate ▁█
wandb:                             train/extr_return_raw_mag █▁
wandb:                             train/extr_return_raw_max █▁
wandb:                            train/extr_return_raw_mean █▁
wandb:                             train/extr_return_raw_min █▁
wandb:                             train/extr_return_raw_std █▁
wandb:                                 train/extr_reward_mag ▁█
wandb:                                 train/extr_reward_max ▁█
wandb:                                train/extr_reward_mean █▁
wandb:                                 train/extr_reward_min ▁█
wandb:                                 train/extr_reward_std █▁
wandb:                                 train/image_loss_mean █▁
wandb:                                  train/image_loss_std █▁
wandb:                                 train/model_loss_mean █▁
wandb:                                  train/model_loss_std █▁
wandb:                             train/model_opt_grad_norm ▁█
wandb:                            train/model_opt_grad_steps ▁█
wandb:                                  train/model_opt_loss █▁
wandb:               train/model_opt_model_opt_grad_overflow ▁▁
wandb:                  train/model_opt_model_opt_grad_scale █▁
wandb:         train/params_agent/task_behavior/ac/actor_opt ▁
wandb:    train/params_agent/task_behavior/critic/critic_opt ▁
wandb:                       train/params_agent/wm/model_opt ▁
wandb:                              train/policy_entropy_mag ▁█
wandb:                              train/policy_entropy_max ▁█
wandb:                             train/policy_entropy_mean ▁█
wandb:                              train/policy_entropy_min ▁█
wandb:                              train/policy_entropy_std ▁█
wandb:                              train/policy_logprob_mag █▁
wandb:                              train/policy_logprob_max █▁
wandb:                             train/policy_logprob_mean █▁
wandb:                              train/policy_logprob_min ▁█
wandb:                              train/policy_logprob_std ▁█
wandb:                           train/policy_randomness_mag ▁█
wandb:                           train/policy_randomness_max ▁█
wandb:                          train/policy_randomness_mean ▁█
wandb:                           train/policy_randomness_min ▁█
wandb:                           train/policy_randomness_std ▁█
wandb:                                    train/post_ent_mag █▁
wandb:                                    train/post_ent_max █▁
wandb:                                   train/post_ent_mean ▁█
wandb:                                    train/post_ent_min ▁█
wandb:                                    train/post_ent_std █▁
wandb:                                   train/prior_ent_mag ▁█
wandb:                                   train/prior_ent_max ▁█
wandb:                                  train/prior_ent_mean ▁█
wandb:                                   train/prior_ent_min ▁█
wandb:                                   train/prior_ent_std █▁
wandb:                                   train/rep_loss_mean █▁
wandb:                                    train/rep_loss_std █▁
wandb:                                      train/reward_avg ▁█
wandb:                                train/reward_loss_mean █▁
wandb:                                 train/reward_loss_std █▁
wandb:                                 train/reward_max_data ▁▁
wandb:                                 train/reward_max_pred ▁█
wandb:                                  train/reward_neg_acc ▁█
wandb:                                 train/reward_neg_loss █▁
wandb:                                  train/reward_pos_acc ▁█
wandb:                                 train/reward_pos_loss █▁
wandb:                                     train/reward_pred █▁
wandb:                                     train/reward_rate ▁█
wandb:                     train/transition_tokens_loss_mean █▁
wandb:                      train/transition_tokens_loss_std █▁
wandb:         train_stats/max_log_achievement_collect_drink ▁
wandb:       train_stats/max_log_achievement_collect_sapling ▁
wandb:          train_stats/max_log_achievement_collect_wood ▁
wandb:       train_stats/max_log_achievement_make_wood_sword ▁
wandb:           train_stats/max_log_achievement_place_plant ▁
wandb:           train_stats/max_log_achievement_place_table ▁
wandb:               train_stats/max_log_achievement_wake_up ▁
wandb:                          train_stats/mean_log_entropy ▁
wandb:                            train_stats/sum_log_reward ▁
wandb: 
wandb: Run summary:
wandb:                                        episode/length 176.0
wandb:                                   episode/reward_rate 0.99435
wandb:                                         episode/score 4.1
wandb:                                         eval/cont_avg 0.99414
wandb:                                   eval/cont_loss_mean 0.00022
wandb:                                    eval/cont_loss_std 0.00436
wandb:                                     eval/cont_neg_acc 1.0
wandb:                                    eval/cont_neg_loss 0.01301
wandb:                                     eval/cont_pos_acc 1.0
wandb:                                    eval/cont_pos_loss 0.00014
wandb:                                        eval/cont_pred 0.99408
wandb:                                        eval/cont_rate 0.99414
wandb:                                    eval/dyn_loss_mean 22.21645
wandb:                                     eval/dyn_loss_std 11.43543
wandb:                                  eval/image_loss_mean 46.80483
wandb:                                   eval/image_loss_std 46.02252
wandb:                                  eval/model_loss_mean 60.23484
wandb:                                   eval/model_loss_std 48.97014
wandb:                                     eval/post_ent_mag 37.86929
wandb:                                     eval/post_ent_max 37.86929
wandb:                                    eval/post_ent_mean 25.16511
wandb:                                     eval/post_ent_min 10.28326
wandb:                                     eval/post_ent_std 4.87627
wandb:                                    eval/prior_ent_mag 63.20827
wandb:                                    eval/prior_ent_max 63.20827
wandb:                                   eval/prior_ent_mean 32.02894
wandb:                                    eval/prior_ent_min 11.23021
wandb:                                    eval/prior_ent_std 9.12445
wandb:                                    eval/rep_loss_mean 22.21645
wandb:                                     eval/rep_loss_std 11.43543
wandb:                                       eval/reward_avg 0.00791
wandb:                                 eval/reward_loss_mean 0.09908
wandb:                                  eval/reward_loss_std 0.67396
wandb:                                  eval/reward_max_data 1.0
wandb:                                  eval/reward_max_pred 1.01071
wandb:                                   eval/reward_neg_acc 1.0
wandb:                                  eval/reward_neg_loss 0.07863
wandb:                                   eval/reward_pos_acc 0.84615
wandb:                                  eval/reward_pos_loss 1.68883
wandb:                                      eval/reward_pred 0.00604
wandb:                                      eval/reward_rate 0.0127
wandb:                      eval/transition_tokens_loss_mean 0.00085
wandb:                       eval/transition_tokens_loss_std 3e-05
wandb:                                   eval_episode/length 39.0
wandb:                              eval_episode/reward_rate 0.975
wandb:                                    eval_episode/score 1.1
wandb:                                   eval_replay/inserts 0.0
wandb:                           eval_replay/sample_wait_avg 0.0
wandb:                          eval_replay/sample_wait_frac 1.0
wandb:                                   eval_replay/samples 16.0
wandb:                                      eval_replay/size 2692.0
wandb:        eval_stats/max_log_achievement_collect_sapling 1.0
wandb:            eval_stats/max_log_achievement_place_plant 1.0
wandb:                             eval_stats/sum_log_reward 1.1
wandb:                                                   fps 3.02993
wandb:                                           global_step 20445
wandb:                                replay/insert_wait_avg 0.0
wandb:                               replay/insert_wait_frac 1.0
wandb:                                        replay/inserts 782.0
wandb:                                replay/sample_wait_avg 0.0
wandb:                               replay/sample_wait_frac 1.0
wandb:                                        replay/samples 6752.0
wandb:                                           replay/size 55542.0
wandb:                                       report/cont_avg 0.99121
wandb:                                 report/cont_loss_mean 7e-05
wandb:                                  report/cont_loss_std 0.00096
wandb:                                   report/cont_neg_acc 1.0
wandb:                                  report/cont_neg_loss 0.00018
wandb:                                   report/cont_pos_acc 1.0
wandb:                                  report/cont_pos_loss 7e-05
wandb:                                      report/cont_pred 0.99114
wandb:                                      report/cont_rate 0.99121
wandb:                                  report/dyn_loss_mean 4.3679
wandb:                                   report/dyn_loss_std 7.47422
wandb:                                report/image_loss_mean 5.40474
wandb:                                 report/image_loss_std 7.99068
wandb:                                report/model_loss_mean 8.09722
wandb:                                 report/model_loss_std 11.14869
wandb:                                   report/post_ent_mag 39.3064
wandb:                                   report/post_ent_max 39.3064
wandb:                                  report/post_ent_mean 23.08085
wandb:                                   report/post_ent_min 10.86538
wandb:                                   report/post_ent_std 5.17416
wandb:                                  report/prior_ent_mag 63.20827
wandb:                                  report/prior_ent_max 63.20827
wandb:                                 report/prior_ent_mean 27.93421
wandb:                                  report/prior_ent_min 13.25203
wandb:                                  report/prior_ent_std 9.25768
wandb:                                  report/rep_loss_mean 4.3679
wandb:                                   report/rep_loss_std 7.47422
wandb:                                     report/reward_avg 0.02598
wandb:                               report/reward_loss_mean 0.07082
wandb:                                report/reward_loss_std 0.37119
wandb:                                report/reward_max_data 1.0
wandb:                                report/reward_max_pred 1.00285
wandb:                                 report/reward_neg_acc 0.99798
wandb:                                report/reward_neg_loss 0.0494
wandb:                                 report/reward_pos_acc 1.0
wandb:                                report/reward_pos_loss 0.71414
wandb:                                    report/reward_pred 0.02627
wandb:                                    report/reward_rate 0.03223
wandb:                    report/transition_tokens_loss_mean 0.00086
wandb:                     report/transition_tokens_loss_std 3e-05
wandb:                                timer/agent.policy_avg 0.01031
wandb:                              timer/agent.policy_count 844.0
wandb:                               timer/agent.policy_frac 0.03124
wandb:                                timer/agent.policy_max 0.02064
wandb:                                timer/agent.policy_min 0.00842
wandb:                              timer/agent.policy_total 8.70121
wandb:                                timer/agent.report_avg 0.22863
wandb:                              timer/agent.report_count 2.0
wandb:                               timer/agent.report_frac 0.00164
wandb:                                timer/agent.report_max 0.22945
wandb:                                timer/agent.report_min 0.22781
wandb:                              timer/agent.report_total 0.45725
wandb:                                 timer/agent.train_avg 0.4034
wandb:                               timer/agent.train_count 422.0
wandb:                                timer/agent.train_frac 0.61115
wandb:                                 timer/agent.train_max 0.42927
wandb:                                 timer/agent.train_min 0.3942
wandb:                               timer/agent.train_total 170.23555
wandb:                                timer/dataset_eval_avg 6e-05
wandb:                              timer/dataset_eval_count 1.0
wandb:                               timer/dataset_eval_frac 0.0
wandb:                                timer/dataset_eval_max 6e-05
wandb:                                timer/dataset_eval_min 6e-05
wandb:                              timer/dataset_eval_total 6e-05
wandb:                               timer/dataset_train_avg 0.00013
wandb:                             timer/dataset_train_count 422.0
wandb:                              timer/dataset_train_frac 0.0002
wandb:                               timer/dataset_train_max 0.00055
wandb:                               timer/dataset_train_min 0.00011
wandb:                             timer/dataset_train_total 0.05602
wandb:                                        timer/duration 278.55096
wandb:                                    timer/env.step_avg 0.10313
wandb:                                  timer/env.step_count 844.0
wandb:                                   timer/env.step_frac 0.31247
wandb:                                    timer/env.step_max 1.37435
wandb:                                    timer/env.step_min 0.00256
wandb:                                  timer/env.step_total 87.03918
wandb:                              timer/replay._sample_avg 0.05438
wandb:                            timer/replay._sample_count 6752.0
wandb:                             timer/replay._sample_frac 1.31812
wandb:                              timer/replay._sample_max 0.09875
wandb:                              timer/replay._sample_min 0.00068
wandb:                            timer/replay._sample_total 367.16449
wandb:                                      train/action_mag 16.0
wandb:                                      train/action_max 16.0
wandb:                                     train/action_mean 6.85412
wandb:                                      train/action_min 0.0
wandb:                                      train/action_std 4.94766
wandb:               train/actor_opt_actor_opt_grad_overflow 0.0
wandb:                  train/actor_opt_actor_opt_grad_scale 10000.0
wandb:                             train/actor_opt_grad_norm 0.03941
wandb:                            train/actor_opt_grad_steps 9915.0
wandb:                                  train/actor_opt_loss -34.1456
wandb:                                         train/adv_mag 0.7724
wandb:                                         train/adv_max 0.74662
wandb:                                        train/adv_mean -0.00145
wandb:                                         train/adv_min -0.55987
wandb:                                         train/adv_std 0.06126
wandb:                                        train/cont_avg 0.9944
wandb:                                  train/cont_loss_mean 0.00016
wandb:                                   train/cont_loss_std 0.00424
wandb:                                    train/cont_neg_acc 0.99405
wandb:                                   train/cont_neg_loss 0.01767
wandb:                                    train/cont_pos_acc 0.99998
wandb:                                   train/cont_pos_loss 8e-05
wandb:                                       train/cont_pred 0.99438
wandb:                                       train/cont_rate 0.9944
wandb:                                   train/dyn_loss_mean 4.58948
wandb:                                    train/dyn_loss_std 7.42146
wandb: train/extr_critic_critic_opt_critic_opt_grad_overflow 0.0
wandb:    train/extr_critic_critic_opt_critic_opt_grad_scale 10000.0
wandb:                train/extr_critic_critic_opt_grad_norm 1.41307
wandb:               train/extr_critic_critic_opt_grad_steps 9915.0
wandb:                     train/extr_critic_critic_opt_loss 16261.33398
wandb:                                 train/extr_critic_mag 8.98025
wandb:                                 train/extr_critic_max 8.98025
wandb:                                train/extr_critic_mean 1.5849
wandb:                                 train/extr_critic_min -0.52683
wandb:                                 train/extr_critic_std 1.75469
wandb:                          train/extr_return_normed_mag 1.7305
wandb:                          train/extr_return_normed_max 1.7305
wandb:                         train/extr_return_normed_mean 0.31917
wandb:                          train/extr_return_normed_min -0.1332
wandb:                          train/extr_return_normed_std 0.31894
wandb:                                train/extr_return_rate 0.62814
wandb:                             train/extr_return_raw_mag 9.45201
wandb:                             train/extr_return_raw_max 9.45201
wandb:                            train/extr_return_raw_mean 1.57651
wandb:                             train/extr_return_raw_min -0.94976
wandb:                             train/extr_return_raw_std 1.78549
wandb:                                 train/extr_reward_mag 1.01688
wandb:                                 train/extr_reward_max 1.01688
wandb:                                train/extr_reward_mean 0.02026
wandb:                                 train/extr_reward_min -0.68281
wandb:                                 train/extr_reward_std 0.14436
wandb:                                 train/image_loss_mean 6.60088
wandb:                                  train/image_loss_std 11.16523
wandb:                                 train/model_loss_mean 9.39445
wandb:                                  train/model_loss_std 14.29037
wandb:                             train/model_opt_grad_norm 91.53799
wandb:                            train/model_opt_grad_steps 9901.07129
wandb:                                  train/model_opt_loss 3319.31885
wandb:               train/model_opt_model_opt_grad_overflow 0.0
wandb:                  train/model_opt_model_opt_grad_scale 334.82144
wandb:         train/params_agent/task_behavior/ac/actor_opt 9464849.0
wandb:    train/params_agent/task_behavior/critic/critic_opt 9708799.0
wandb:                       train/params_agent/wm/model_opt 197057280.0
wandb:                              train/policy_entropy_mag 2.42722
wandb:                              train/policy_entropy_max 2.42722
wandb:                             train/policy_entropy_mean 0.71452
wandb:                              train/policy_entropy_min 0.07938
wandb:                              train/policy_entropy_std 0.57643
wandb:                              train/policy_logprob_mag 7.43837
wandb:                              train/policy_logprob_max -0.00946
wandb:                             train/policy_logprob_mean -0.71502
wandb:                              train/policy_logprob_min -7.43837
wandb:                              train/policy_logprob_std 1.18306
wandb:                           train/policy_randomness_mag 0.8567
wandb:                           train/policy_randomness_max 0.8567
wandb:                          train/policy_randomness_mean 0.2522
wandb:                           train/policy_randomness_min 0.02802
wandb:                           train/policy_randomness_std 0.20345
wandb:                                    train/post_ent_mag 37.24263
wandb:                                    train/post_ent_max 37.24263
wandb:                                   train/post_ent_mean 22.9007
wandb:                                    train/post_ent_min 9.74037
wandb:                                    train/post_ent_std 4.77855
wandb:                                   train/prior_ent_mag 64.81831
wandb:                                   train/prior_ent_max 64.81831
wandb:                                  train/prior_ent_mean 27.50653
wandb:                                   train/prior_ent_min 11.6162
wandb:                                   train/prior_ent_std 8.653
wandb:                                   train/rep_loss_mean 4.58948
wandb:                                    train/rep_loss_std 7.42146
wandb:                                      train/reward_avg 0.01451
wandb:                                train/reward_loss_mean 0.03909
wandb:                                 train/reward_loss_std 0.19711
wandb:                                 train/reward_max_data 1.0
wandb:                                 train/reward_max_pred 1.00311
wandb:                                  train/reward_neg_acc 0.99687
wandb:                                 train/reward_neg_loss 0.02421
wandb:                                  train/reward_pos_acc 0.97965
wandb:                                 train/reward_pos_loss 0.78233
wandb:                                     train/reward_pred 0.01417
wandb:                                     train/reward_rate 0.0196
wandb:                     train/transition_tokens_loss_mean 0.00065
wandb:                      train/transition_tokens_loss_std 3e-05
wandb:         train_stats/max_log_achievement_collect_drink 11.5
wandb:       train_stats/max_log_achievement_collect_sapling 0.66667
wandb:          train_stats/max_log_achievement_collect_wood 3.5
wandb:       train_stats/max_log_achievement_make_wood_sword 1.0
wandb:           train_stats/max_log_achievement_place_plant 0.66667
wandb:           train_stats/max_log_achievement_place_table 1.0
wandb:               train_stats/max_log_achievement_wake_up 3.33333
wandb:                          train_stats/mean_log_entropy 0.74073
wandb:                            train_stats/sum_log_reward 3.43333
wandb: 
wandb: 🚀 View run ziyu_crafter_cuda_3_seed_3 at: https://wandb.ai/ibisbill326-gmail-com/crafter_reward/runs/a4qs02g3
wandb: Synced 3 W&B file(s), 6 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240102_174011-a4qs02g3/logs
